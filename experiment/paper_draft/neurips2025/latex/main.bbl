\begin{thebibliography}{10}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2024)Chen, Zhu, Shi, Zhang, Chen, Ding, Li, and
  Wan]{logicvqa}
W.~Chen, X.~Zhu, M.~Shi, Y.~Zhang, W.~Chen, Z.~Ding, Y.~Li, and X.~Wan.
\newblock Can {VLMs} see beyond visual perception? {A} case study on complex
  reasoning.
\newblock \emph{arXiv preprint arXiv:2402.12982}, 2024.

\bibitem[Das et~al.(2024)Das, Camburu, Aralikatte, Ivanova, Kursun, Barrow,
  Vedantam, Lee, and Lee]{examsv}
R.~J. Das, O.-M. Camburu, R.~Aralikatte, E.~Ivanova, O.~Kursun, J.~Barrow,
  R.~Vedantam, K.-W. Lee, and N.~Lee.
\newblock {EXAMS-V}: A multi-discipline multilingual multimodal exam benchmark
  for evaluating vision language models.
\newblock \emph{arXiv preprint arXiv:2403.10378}, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{mmlu}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and
  J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Dou, Wang, Zhang, Wang, and
  Fan]{multilingual_vlm}
Y.~Liu, Y.~Dou, Y.~Wang, K.~Zhang, Y.~Wang, and W.~Fan.
\newblock Multilingual evaluation of vision-language models.
\newblock \emph{arXiv preprint}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Li, Bai, Liu, Liu, Zhang, Zheng,
  and Lin]{ocrbench}
Y.~Liu, Z.~Li, H.~Bai, Y.~Liu, J.~Liu, C.~Zhang, Z.~Zheng, and B.~Lin.
\newblock {OCRBench}: On the hidden mystery of {OCR} in large multimodal
  models.
\newblock \emph{arXiv preprint arXiv:2305.07895}, 2024{\natexlab{b}}.

\bibitem[Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang,
  Galley, and Gao]{mathvista}
P.~Lu, H.~Bansal, T.~Xia, J.~Liu, C.~Li, H.~Hajishirzi, H.~Cheng, K.-W. Chang,
  M.~Galley, and J.~Gao.
\newblock {MathVista}: Evaluating mathematical reasoning of foundation models
  in visual contexts.
\newblock \emph{arXiv preprint arXiv:2310.02255}, 2023.

\bibitem[Masry et~al.(2022)Masry, Long, Tan, Joty, and Hoque]{chartqa}
A.~Masry, D.~X. Long, J.~Q. Tan, S.~Joty, and E.~Hoque.
\newblock {ChartQA}: A benchmark for question answering about charts with
  visual and logical reasoning.
\newblock \emph{arXiv preprint arXiv:2203.10244}, 2022.

\bibitem[Mathew et~al.(2021)Mathew, Karatzas, and Jawahar]{docvqa}
M.~Mathew, D.~Karatzas, and C.~Jawahar.
\newblock {DocVQA}: A dataset for {VQA} on document images.
\newblock \emph{Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision}, pages 2200--2209, 2021.

\bibitem[Singh et~al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
  and Rohrbach]{textvqa}
A.~Singh, V.~Natarajan, M.~Shah, Y.~Jiang, X.~Chen, D.~Batra, D.~Parikh, and
  M.~Rohrbach.
\newblock {TextVQA}: Towards reading text in images to answer questions.
\newblock \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8317--8326, 2019.

\bibitem[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang,
  Ren, Sun, et~al.]{mmmu}
X.~Yue, Y.~Ni, K.~Zhang, T.~Zheng, R.~Liu, G.~Zhang, S.~Stevens, D.~Jiang,
  W.~Ren, Y.~Sun, et~al.
\newblock {MMMU}: A massive multi-discipline multimodal understanding and
  reasoning benchmark for expert {AGI}.
\newblock \emph{arXiv preprint arXiv:2311.16502}, 2023.

\end{thebibliography}
