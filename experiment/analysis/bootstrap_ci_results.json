[
  {
    "model": "o3",
    "overall": {
      "accuracy": 84.26,
      "n": 8000,
      "ci_lower": 83.45,
      "ci_upper": 85.04,
      "ci_width": 1.6
    },
    "by_nation": {
      "india": {
        "accuracy": 68.64,
        "n": 1699,
        "ci_lower": 66.39,
        "ci_upper": 70.8,
        "ci_width": 4.41
      },
      "eu": {
        "accuracy": 84.49,
        "n": 1289,
        "ci_lower": 82.41,
        "ci_upper": 86.36,
        "ci_width": 3.95
      },
      "taiwan": {
        "accuracy": 93.72,
        "n": 557,
        "ci_lower": 91.39,
        "ci_upper": 95.45,
        "ci_width": 4.06
      },
      "japan": {
        "accuracy": 82.37,
        "n": 2557,
        "ci_lower": 80.84,
        "ci_upper": 83.8,
        "ci_width": 2.95
      },
      "south_korea": {
        "accuracy": 90.06,
        "n": 1898,
        "ci_lower": 88.63,
        "ci_upper": 91.33,
        "ci_width": 2.69
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 85.51,
        "n": 207,
        "ci_lower": 80.07,
        "ci_upper": 89.66,
        "ci_width": 9.59
      },
      "philosophy": {
        "accuracy": 90.55,
        "n": 127,
        "ci_lower": 84.21,
        "ci_upper": 94.51,
        "ci_width": 10.3
      },
      "earth_science": {
        "accuracy": 73.83,
        "n": 107,
        "ci_lower": 64.78,
        "ci_upper": 81.23,
        "ci_width": 16.45
      },
      "psychology": {
        "accuracy": 92.44,
        "n": 119,
        "ci_lower": 86.25,
        "ci_upper": 95.97,
        "ci_width": 9.72
      },
      "economics": {
        "accuracy": 87.2,
        "n": 463,
        "ci_lower": 83.85,
        "ci_upper": 89.94,
        "ci_width": 6.09
      },
      "biology": {
        "accuracy": 89.23,
        "n": 899,
        "ci_lower": 87.03,
        "ci_upper": 91.09,
        "ci_width": 4.06
      },
      "geography": {
        "accuracy": 74.03,
        "n": 181,
        "ci_lower": 67.19,
        "ci_upper": 79.87,
        "ci_width": 12.68
      },
      "physics": {
        "accuracy": 81.85,
        "n": 540,
        "ci_lower": 78.38,
        "ci_upper": 84.87,
        "ci_width": 6.49
      },
      "politics": {
        "accuracy": 79.9,
        "n": 209,
        "ci_lower": 73.95,
        "ci_upper": 84.77,
        "ci_width": 10.82
      },
      "history": {
        "accuracy": 86.33,
        "n": 256,
        "ci_lower": 81.58,
        "ci_upper": 90.0,
        "ci_width": 8.42
      },
      "administration": {
        "accuracy": 81.53,
        "n": 928,
        "ci_lower": 78.91,
        "ci_upper": 83.89,
        "ci_width": 4.99
      },
      "language": {
        "accuracy": 91.49,
        "n": 740,
        "ci_lower": 89.26,
        "ci_upper": 93.29,
        "ci_width": 4.03
      },
      "medicine": {
        "accuracy": 91.18,
        "n": 238,
        "ci_lower": 86.89,
        "ci_upper": 94.16,
        "ci_width": 7.27
      },
      "engineering": {
        "accuracy": 84.16,
        "n": 688,
        "ci_lower": 81.24,
        "ci_upper": 86.7,
        "ci_width": 5.45
      },
      "computer_science": {
        "accuracy": 90.93,
        "n": 441,
        "ci_lower": 87.88,
        "ci_upper": 93.27,
        "ci_width": 5.38
      },
      "law": {
        "accuracy": 78.08,
        "n": 818,
        "ci_lower": 75.12,
        "ci_upper": 80.78,
        "ci_width": 5.66
      },
      "mathematics": {
        "accuracy": 78.98,
        "n": 1039,
        "ci_lower": 76.4,
        "ci_upper": 81.35,
        "ci_width": 4.95
      }
    }
  },
  {
    "model": "o4-mini",
    "overall": {
      "accuracy": 79.4,
      "n": 8000,
      "ci_lower": 78.5,
      "ci_upper": 80.27,
      "ci_width": 1.77
    },
    "by_nation": {
      "india": {
        "accuracy": 63.38,
        "n": 1699,
        "ci_lower": 61.06,
        "ci_upper": 65.64,
        "ci_width": 4.58
      },
      "eu": {
        "accuracy": 76.95,
        "n": 1289,
        "ci_lower": 74.57,
        "ci_upper": 79.17,
        "ci_width": 4.59
      },
      "taiwan": {
        "accuracy": 92.29,
        "n": 557,
        "ci_lower": 89.77,
        "ci_upper": 94.23,
        "ci_width": 4.45
      },
      "japan": {
        "accuracy": 82.52,
        "n": 2557,
        "ci_lower": 81.0,
        "ci_upper": 83.94,
        "ci_width": 2.94
      },
      "south_korea": {
        "accuracy": 82.49,
        "n": 1898,
        "ci_lower": 80.72,
        "ci_upper": 84.13,
        "ci_width": 3.42
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 85.02,
        "n": 207,
        "ci_lower": 79.52,
        "ci_upper": 89.24,
        "ci_width": 9.72
      },
      "philosophy": {
        "accuracy": 85.83,
        "n": 127,
        "ci_lower": 78.71,
        "ci_upper": 90.85,
        "ci_width": 12.14
      },
      "earth_science": {
        "accuracy": 75.7,
        "n": 107,
        "ci_lower": 66.78,
        "ci_upper": 82.84,
        "ci_width": 16.07
      },
      "psychology": {
        "accuracy": 85.71,
        "n": 119,
        "ci_lower": 78.3,
        "ci_upper": 90.88,
        "ci_width": 12.58
      },
      "economics": {
        "accuracy": 82.86,
        "n": 463,
        "ci_lower": 79.16,
        "ci_upper": 86.02,
        "ci_width": 6.86
      },
      "biology": {
        "accuracy": 83.02,
        "n": 899,
        "ci_lower": 80.43,
        "ci_upper": 85.33,
        "ci_width": 4.91
      },
      "geography": {
        "accuracy": 64.09,
        "n": 181,
        "ci_lower": 56.87,
        "ci_upper": 70.72,
        "ci_width": 13.84
      },
      "physics": {
        "accuracy": 89.63,
        "n": 540,
        "ci_lower": 86.77,
        "ci_upper": 91.93,
        "ci_width": 5.16
      },
      "politics": {
        "accuracy": 73.21,
        "n": 209,
        "ci_lower": 66.83,
        "ci_upper": 78.76,
        "ci_width": 11.93
      },
      "history": {
        "accuracy": 67.97,
        "n": 256,
        "ci_lower": 62.02,
        "ci_upper": 73.38,
        "ci_width": 11.36
      },
      "administration": {
        "accuracy": 71.21,
        "n": 928,
        "ci_lower": 68.21,
        "ci_upper": 74.03,
        "ci_width": 5.82
      },
      "language": {
        "accuracy": 84.19,
        "n": 740,
        "ci_lower": 81.39,
        "ci_upper": 86.64,
        "ci_width": 5.26
      },
      "medicine": {
        "accuracy": 88.24,
        "n": 238,
        "ci_lower": 83.53,
        "ci_upper": 91.74,
        "ci_width": 8.21
      },
      "engineering": {
        "accuracy": 81.98,
        "n": 688,
        "ci_lower": 78.93,
        "ci_upper": 84.67,
        "ci_width": 5.74
      },
      "computer_science": {
        "accuracy": 92.52,
        "n": 441,
        "ci_lower": 89.68,
        "ci_upper": 94.62,
        "ci_width": 4.94
      },
      "law": {
        "accuracy": 63.92,
        "n": 818,
        "ci_lower": 60.57,
        "ci_upper": 67.14,
        "ci_width": 6.57
      },
      "mathematics": {
        "accuracy": 80.61,
        "n": 1039,
        "ci_lower": 78.1,
        "ci_upper": 82.9,
        "ci_width": 4.8
      }
    }
  },
  {
    "model": "GPT-4o",
    "overall": {
      "accuracy": 42.04,
      "n": 8000,
      "ci_lower": 40.96,
      "ci_upper": 43.13,
      "ci_width": 2.16
    },
    "by_nation": {
      "india": {
        "accuracy": 40.99,
        "n": 1699,
        "ci_lower": 38.67,
        "ci_upper": 43.35,
        "ci_width": 4.67
      },
      "eu": {
        "accuracy": 63.73,
        "n": 1289,
        "ci_lower": 61.07,
        "ci_upper": 66.31,
        "ci_width": 5.24
      },
      "taiwan": {
        "accuracy": 66.66,
        "n": 557,
        "ci_lower": 62.64,
        "ci_upper": 70.45,
        "ci_width": 7.81
      },
      "japan": {
        "accuracy": 25.97,
        "n": 2557,
        "ci_lower": 24.31,
        "ci_upper": 27.7,
        "ci_width": 3.4
      },
      "south_korea": {
        "accuracy": 33.25,
        "n": 1898,
        "ci_lower": 31.17,
        "ci_upper": 35.4,
        "ci_width": 4.24
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 50.72,
        "n": 207,
        "ci_lower": 43.96,
        "ci_upper": 57.46,
        "ci_width": 13.5
      },
      "philosophy": {
        "accuracy": 36.22,
        "n": 127,
        "ci_lower": 28.38,
        "ci_upper": 44.87,
        "ci_width": 16.49
      },
      "earth_science": {
        "accuracy": 17.76,
        "n": 107,
        "ci_lower": 11.68,
        "ci_upper": 26.08,
        "ci_width": 14.4
      },
      "psychology": {
        "accuracy": 29.41,
        "n": 119,
        "ci_lower": 21.97,
        "ci_upper": 38.14,
        "ci_width": 16.17
      },
      "economics": {
        "accuracy": 31.89,
        "n": 463,
        "ci_lower": 27.81,
        "ci_upper": 36.27,
        "ci_width": 8.46
      },
      "biology": {
        "accuracy": 42.73,
        "n": 899,
        "ci_lower": 39.53,
        "ci_upper": 45.99,
        "ci_width": 6.45
      },
      "geography": {
        "accuracy": 37.02,
        "n": 181,
        "ci_lower": 30.32,
        "ci_upper": 44.26,
        "ci_width": 13.93
      },
      "physics": {
        "accuracy": 33.52,
        "n": 540,
        "ci_lower": 29.67,
        "ci_upper": 37.61,
        "ci_width": 7.94
      },
      "politics": {
        "accuracy": 34.93,
        "n": 209,
        "ci_lower": 28.79,
        "ci_upper": 41.61,
        "ci_width": 12.82
      },
      "history": {
        "accuracy": 35.55,
        "n": 256,
        "ci_lower": 29.94,
        "ci_upper": 41.59,
        "ci_width": 11.65
      },
      "administration": {
        "accuracy": 47.48,
        "n": 928,
        "ci_lower": 44.28,
        "ci_upper": 50.7,
        "ci_width": 6.41
      },
      "language": {
        "accuracy": 49.73,
        "n": 740,
        "ci_lower": 46.14,
        "ci_upper": 53.32,
        "ci_width": 7.19
      },
      "medicine": {
        "accuracy": 50.0,
        "n": 238,
        "ci_lower": 43.7,
        "ci_upper": 56.3,
        "ci_width": 12.6
      },
      "engineering": {
        "accuracy": 33.72,
        "n": 688,
        "ci_lower": 30.29,
        "ci_upper": 37.33,
        "ci_width": 7.05
      },
      "computer_science": {
        "accuracy": 58.96,
        "n": 441,
        "ci_lower": 54.31,
        "ci_upper": 63.45,
        "ci_width": 9.14
      },
      "law": {
        "accuracy": 42.0,
        "n": 818,
        "ci_lower": 38.66,
        "ci_upper": 45.41,
        "ci_width": 6.75
      },
      "mathematics": {
        "accuracy": 43.38,
        "n": 1039,
        "ci_lower": 40.4,
        "ci_upper": 46.41,
        "ci_width": 6.02
      }
    }
  },
  {
    "model": "GPT-4.1",
    "overall": {
      "accuracy": 54.73,
      "n": 8000,
      "ci_lower": 53.64,
      "ci_upper": 55.82,
      "ci_width": 2.18
    },
    "by_nation": {
      "india": {
        "accuracy": 48.1,
        "n": 1699,
        "ci_lower": 45.73,
        "ci_upper": 50.48,
        "ci_width": 4.75
      },
      "eu": {
        "accuracy": 66.44,
        "n": 1289,
        "ci_lower": 63.82,
        "ci_upper": 68.97,
        "ci_width": 5.15
      },
      "taiwan": {
        "accuracy": 72.58,
        "n": 557,
        "ci_lower": 68.73,
        "ci_upper": 76.12,
        "ci_width": 7.39
      },
      "japan": {
        "accuracy": 48.1,
        "n": 2557,
        "ci_lower": 46.17,
        "ci_upper": 50.04,
        "ci_width": 3.87
      },
      "south_korea": {
        "accuracy": 54.23,
        "n": 1898,
        "ci_lower": 51.98,
        "ci_upper": 56.46,
        "ci_width": 4.48
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 68.12,
        "n": 207,
        "ci_lower": 61.49,
        "ci_upper": 74.09,
        "ci_width": 12.6
      },
      "philosophy": {
        "accuracy": 55.12,
        "n": 127,
        "ci_lower": 46.45,
        "ci_upper": 63.49,
        "ci_width": 17.05
      },
      "earth_science": {
        "accuracy": 33.64,
        "n": 107,
        "ci_lower": 25.39,
        "ci_upper": 43.02,
        "ci_width": 17.63
      },
      "psychology": {
        "accuracy": 46.22,
        "n": 119,
        "ci_lower": 37.52,
        "ci_upper": 55.16,
        "ci_width": 17.64
      },
      "economics": {
        "accuracy": 46.42,
        "n": 463,
        "ci_lower": 41.93,
        "ci_upper": 50.97,
        "ci_width": 9.05
      },
      "biology": {
        "accuracy": 53.5,
        "n": 899,
        "ci_lower": 50.23,
        "ci_upper": 56.74,
        "ci_width": 6.51
      },
      "geography": {
        "accuracy": 48.07,
        "n": 181,
        "ci_lower": 40.91,
        "ci_upper": 55.31,
        "ci_width": 14.41
      },
      "physics": {
        "accuracy": 54.63,
        "n": 540,
        "ci_lower": 50.41,
        "ci_upper": 58.78,
        "ci_width": 8.37
      },
      "politics": {
        "accuracy": 45.93,
        "n": 209,
        "ci_lower": 39.31,
        "ci_upper": 52.7,
        "ci_width": 13.39
      },
      "history": {
        "accuracy": 48.83,
        "n": 256,
        "ci_lower": 42.77,
        "ci_upper": 54.93,
        "ci_width": 12.16
      },
      "administration": {
        "accuracy": 54.57,
        "n": 928,
        "ci_lower": 51.35,
        "ci_upper": 57.75,
        "ci_width": 6.39
      },
      "language": {
        "accuracy": 60.14,
        "n": 740,
        "ci_lower": 56.57,
        "ci_upper": 63.61,
        "ci_width": 7.04
      },
      "medicine": {
        "accuracy": 69.75,
        "n": 238,
        "ci_lower": 63.64,
        "ci_upper": 75.23,
        "ci_width": 11.6
      },
      "engineering": {
        "accuracy": 50.0,
        "n": 688,
        "ci_lower": 46.27,
        "ci_upper": 53.73,
        "ci_width": 7.45
      },
      "computer_science": {
        "accuracy": 73.47,
        "n": 441,
        "ci_lower": 69.16,
        "ci_upper": 77.38,
        "ci_width": 8.22
      },
      "law": {
        "accuracy": 47.54,
        "n": 818,
        "ci_lower": 44.14,
        "ci_upper": 50.97,
        "ci_width": 6.83
      },
      "mathematics": {
        "accuracy": 58.06,
        "n": 1039,
        "ci_lower": 55.04,
        "ci_upper": 61.03,
        "ci_width": 5.99
      }
    }
  },
  {
    "model": "GPT-4.1-mini",
    "overall": {
      "accuracy": 56.27,
      "n": 8000,
      "ci_lower": 55.18,
      "ci_upper": 57.35,
      "ci_width": 2.17
    },
    "by_nation": {
      "india": {
        "accuracy": 46.34,
        "n": 1699,
        "ci_lower": 43.98,
        "ci_upper": 48.72,
        "ci_width": 4.74
      },
      "eu": {
        "accuracy": 63.58,
        "n": 1289,
        "ci_lower": 60.92,
        "ci_upper": 66.16,
        "ci_width": 5.25
      },
      "taiwan": {
        "accuracy": 79.03,
        "n": 557,
        "ci_lower": 75.46,
        "ci_upper": 82.21,
        "ci_width": 6.75
      },
      "japan": {
        "accuracy": 43.84,
        "n": 2557,
        "ci_lower": 41.93,
        "ci_upper": 45.77,
        "ci_width": 3.84
      },
      "south_korea": {
        "accuracy": 59.92,
        "n": 1898,
        "ci_lower": 57.7,
        "ci_upper": 62.1,
        "ci_width": 4.41
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 65.22,
        "n": 207,
        "ci_lower": 58.51,
        "ci_upper": 71.38,
        "ci_width": 12.87
      },
      "philosophy": {
        "accuracy": 72.44,
        "n": 127,
        "ci_lower": 64.1,
        "ci_upper": 79.47,
        "ci_width": 15.37
      },
      "earth_science": {
        "accuracy": 34.58,
        "n": 107,
        "ci_lower": 26.24,
        "ci_upper": 43.99,
        "ci_width": 17.74
      },
      "psychology": {
        "accuracy": 52.1,
        "n": 119,
        "ci_lower": 43.2,
        "ci_upper": 60.87,
        "ci_width": 17.67
      },
      "economics": {
        "accuracy": 51.84,
        "n": 463,
        "ci_lower": 47.29,
        "ci_upper": 56.36,
        "ci_width": 9.07
      },
      "biology": {
        "accuracy": 58.93,
        "n": 899,
        "ci_lower": 55.68,
        "ci_upper": 62.1,
        "ci_width": 6.42
      },
      "geography": {
        "accuracy": 37.02,
        "n": 181,
        "ci_lower": 30.32,
        "ci_upper": 44.26,
        "ci_width": 13.93
      },
      "physics": {
        "accuracy": 58.7,
        "n": 540,
        "ci_lower": 54.5,
        "ci_upper": 62.78,
        "ci_width": 8.28
      },
      "politics": {
        "accuracy": 47.37,
        "n": 209,
        "ci_lower": 40.71,
        "ci_upper": 54.13,
        "ci_width": 13.42
      },
      "history": {
        "accuracy": 39.84,
        "n": 256,
        "ci_lower": 34.04,
        "ci_upper": 45.94,
        "ci_width": 11.91
      },
      "administration": {
        "accuracy": 54.14,
        "n": 928,
        "ci_lower": 50.92,
        "ci_upper": 57.32,
        "ci_width": 6.4
      },
      "language": {
        "accuracy": 61.62,
        "n": 740,
        "ci_lower": 58.06,
        "ci_upper": 65.06,
        "ci_width": 6.99
      },
      "medicine": {
        "accuracy": 72.69,
        "n": 238,
        "ci_lower": 66.7,
        "ci_upper": 77.96,
        "ci_width": 11.25
      },
      "engineering": {
        "accuracy": 51.16,
        "n": 688,
        "ci_lower": 47.43,
        "ci_upper": 54.88,
        "ci_width": 7.45
      },
      "computer_science": {
        "accuracy": 75.74,
        "n": 441,
        "ci_lower": 71.53,
        "ci_upper": 79.51,
        "ci_width": 7.98
      },
      "law": {
        "accuracy": 51.23,
        "n": 818,
        "ci_lower": 47.81,
        "ci_upper": 54.64,
        "ci_width": 6.83
      },
      "mathematics": {
        "accuracy": 56.24,
        "n": 1039,
        "ci_lower": 53.21,
        "ci_upper": 59.23,
        "ci_width": 6.02
      }
    }
  },
  {
    "model": "Gemini-2.5-pro",
    "overall": {
      "accuracy": 86.99,
      "n": 8000,
      "ci_lower": 86.24,
      "ci_upper": 87.71,
      "ci_width": 1.47
    },
    "by_nation": {
      "india": {
        "accuracy": 69.23,
        "n": 1699,
        "ci_lower": 66.99,
        "ci_upper": 71.38,
        "ci_width": 4.39
      },
      "eu": {
        "accuracy": 88.08,
        "n": 1289,
        "ci_lower": 86.2,
        "ci_upper": 89.74,
        "ci_width": 3.54
      },
      "taiwan": {
        "accuracy": 95.51,
        "n": 557,
        "ci_lower": 93.46,
        "ci_upper": 96.94,
        "ci_width": 3.48
      },
      "japan": {
        "accuracy": 87.59,
        "n": 2557,
        "ci_lower": 86.26,
        "ci_upper": 88.81,
        "ci_width": 2.56
      },
      "south_korea": {
        "accuracy": 91.12,
        "n": 1898,
        "ci_lower": 89.76,
        "ci_upper": 92.32,
        "ci_width": 2.56
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 87.92,
        "n": 207,
        "ci_lower": 82.78,
        "ci_upper": 91.68,
        "ci_width": 8.91
      },
      "philosophy": {
        "accuracy": 93.7,
        "n": 127,
        "ci_lower": 88.06,
        "ci_upper": 96.77,
        "ci_width": 8.71
      },
      "earth_science": {
        "accuracy": 85.98,
        "n": 107,
        "ci_lower": 78.15,
        "ci_upper": 91.32,
        "ci_width": 13.17
      },
      "psychology": {
        "accuracy": 95.8,
        "n": 119,
        "ci_lower": 90.54,
        "ci_upper": 98.19,
        "ci_width": 7.65
      },
      "economics": {
        "accuracy": 88.29,
        "n": 463,
        "ci_lower": 85.04,
        "ci_upper": 90.91,
        "ci_width": 5.87
      },
      "biology": {
        "accuracy": 88.9,
        "n": 899,
        "ci_lower": 86.68,
        "ci_upper": 90.79,
        "ci_width": 4.11
      },
      "geography": {
        "accuracy": 74.03,
        "n": 181,
        "ci_lower": 67.19,
        "ci_upper": 79.87,
        "ci_width": 12.68
      },
      "physics": {
        "accuracy": 84.26,
        "n": 540,
        "ci_lower": 80.95,
        "ci_upper": 87.09,
        "ci_width": 6.14
      },
      "politics": {
        "accuracy": 79.43,
        "n": 209,
        "ci_lower": 73.44,
        "ci_upper": 84.36,
        "ci_width": 10.91
      },
      "history": {
        "accuracy": 87.5,
        "n": 256,
        "ci_lower": 82.89,
        "ci_upper": 91.0,
        "ci_width": 8.12
      },
      "administration": {
        "accuracy": 83.57,
        "n": 928,
        "ci_lower": 81.05,
        "ci_upper": 85.81,
        "ci_width": 4.77
      },
      "language": {
        "accuracy": 91.49,
        "n": 740,
        "ci_lower": 89.26,
        "ci_upper": 93.29,
        "ci_width": 4.03
      },
      "medicine": {
        "accuracy": 95.8,
        "n": 238,
        "ci_lower": 92.44,
        "ci_upper": 97.7,
        "ci_width": 5.26
      },
      "engineering": {
        "accuracy": 86.19,
        "n": 688,
        "ci_lower": 83.41,
        "ci_upper": 88.57,
        "ci_width": 5.16
      },
      "computer_science": {
        "accuracy": 92.06,
        "n": 441,
        "ci_lower": 89.16,
        "ci_upper": 94.24,
        "ci_width": 5.08
      },
      "law": {
        "accuracy": 84.24,
        "n": 818,
        "ci_lower": 81.58,
        "ci_upper": 86.58,
        "ci_width": 4.99
      },
      "mathematics": {
        "accuracy": 86.28,
        "n": 1039,
        "ci_lower": 84.05,
        "ci_upper": 88.24,
        "ci_width": 4.19
      }
    }
  },
  {
    "model": "Gemini-2.5-flash",
    "overall": {
      "accuracy": 68.33,
      "n": 8000,
      "ci_lower": 67.3,
      "ci_upper": 69.34,
      "ci_width": 2.04
    },
    "by_nation": {
      "india": {
        "accuracy": 62.32,
        "n": 1699,
        "ci_lower": 59.99,
        "ci_upper": 64.59,
        "ci_width": 4.6
      },
      "eu": {
        "accuracy": 83.3,
        "n": 1289,
        "ci_lower": 81.17,
        "ci_upper": 85.24,
        "ci_width": 4.07
      },
      "taiwan": {
        "accuracy": 92.65,
        "n": 557,
        "ci_lower": 90.18,
        "ci_upper": 94.54,
        "ci_width": 4.36
      },
      "japan": {
        "accuracy": 51.46,
        "n": 2557,
        "ci_lower": 49.52,
        "ci_upper": 53.39,
        "ci_width": 3.87
      },
      "south_korea": {
        "accuracy": 67.65,
        "n": 1898,
        "ci_lower": 65.51,
        "ci_upper": 69.72,
        "ci_width": 4.21
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 72.95,
        "n": 207,
        "ci_lower": 66.52,
        "ci_upper": 78.54,
        "ci_width": 12.02
      },
      "philosophy": {
        "accuracy": 77.17,
        "n": 127,
        "ci_lower": 69.14,
        "ci_upper": 83.61,
        "ci_width": 14.47
      },
      "earth_science": {
        "accuracy": 48.6,
        "n": 107,
        "ci_lower": 39.34,
        "ci_upper": 57.95,
        "ci_width": 18.61
      },
      "psychology": {
        "accuracy": 66.39,
        "n": 119,
        "ci_lower": 57.51,
        "ci_upper": 74.25,
        "ci_width": 16.74
      },
      "economics": {
        "accuracy": 55.97,
        "n": 463,
        "ci_lower": 51.42,
        "ci_upper": 60.42,
        "ci_width": 9.01
      },
      "biology": {
        "accuracy": 73.36,
        "n": 899,
        "ci_lower": 70.38,
        "ci_upper": 76.15,
        "ci_width": 5.77
      },
      "geography": {
        "accuracy": 54.7,
        "n": 181,
        "ci_lower": 47.43,
        "ci_upper": 61.78,
        "ci_width": 14.35
      },
      "physics": {
        "accuracy": 59.26,
        "n": 540,
        "ci_lower": 55.06,
        "ci_upper": 63.32,
        "ci_width": 8.26
      },
      "politics": {
        "accuracy": 62.2,
        "n": 209,
        "ci_lower": 55.46,
        "ci_upper": 68.5,
        "ci_width": 13.04
      },
      "history": {
        "accuracy": 66.02,
        "n": 256,
        "ci_lower": 60.02,
        "ci_upper": 71.55,
        "ci_width": 11.53
      },
      "administration": {
        "accuracy": 68.53,
        "n": 928,
        "ci_lower": 65.47,
        "ci_upper": 71.44,
        "ci_width": 5.97
      },
      "language": {
        "accuracy": 75.68,
        "n": 740,
        "ci_lower": 72.46,
        "ci_upper": 78.63,
        "ci_width": 6.17
      },
      "medicine": {
        "accuracy": 83.19,
        "n": 238,
        "ci_lower": 77.92,
        "ci_upper": 87.41,
        "ci_width": 9.49
      },
      "engineering": {
        "accuracy": 64.83,
        "n": 688,
        "ci_lower": 61.19,
        "ci_upper": 68.31,
        "ci_width": 7.12
      },
      "computer_science": {
        "accuracy": 75.28,
        "n": 441,
        "ci_lower": 71.05,
        "ci_upper": 79.08,
        "ci_width": 8.03
      },
      "law": {
        "accuracy": 63.18,
        "n": 818,
        "ci_lower": 59.82,
        "ci_upper": 66.42,
        "ci_width": 6.6
      },
      "mathematics": {
        "accuracy": 73.13,
        "n": 1039,
        "ci_lower": 70.35,
        "ci_upper": 75.74,
        "ci_width": 5.38
      }
    }
  },
  {
    "model": "Gemini-2.5-flash-lite",
    "overall": {
      "accuracy": 25.94,
      "n": 8000,
      "ci_lower": 24.99,
      "ci_upper": 26.91,
      "ci_width": 1.92
    },
    "by_nation": {
      "india": {
        "accuracy": 12.95,
        "n": 1699,
        "ci_lower": 11.44,
        "ci_upper": 14.63,
        "ci_width": 3.19
      },
      "eu": {
        "accuracy": 54.37,
        "n": 1289,
        "ci_lower": 51.64,
        "ci_upper": 57.07,
        "ci_width": 5.43
      },
      "taiwan": {
        "accuracy": 73.29,
        "n": 557,
        "ci_lower": 69.47,
        "ci_upper": 76.8,
        "ci_width": 7.33
      },
      "japan": {
        "accuracy": 7.13,
        "n": 2557,
        "ci_lower": 6.2,
        "ci_upper": 8.19,
        "ci_width": 2.0
      },
      "south_korea": {
        "accuracy": 13.99,
        "n": 1898,
        "ci_lower": 12.5,
        "ci_upper": 15.62,
        "ci_width": 3.12
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 22.71,
        "n": 207,
        "ci_lower": 17.53,
        "ci_upper": 28.88,
        "ci_width": 11.35
      },
      "philosophy": {
        "accuracy": 18.11,
        "n": 127,
        "ci_lower": 12.38,
        "ci_upper": 25.71,
        "ci_width": 13.33
      },
      "earth_science": {
        "accuracy": 7.48,
        "n": 107,
        "ci_lower": 3.84,
        "ci_upper": 14.07,
        "ci_width": 10.23
      },
      "psychology": {
        "accuracy": 17.65,
        "n": 119,
        "ci_lower": 11.84,
        "ci_upper": 25.48,
        "ci_width": 13.63
      },
      "economics": {
        "accuracy": 16.49,
        "n": 463,
        "ci_lower": 13.39,
        "ci_upper": 20.14,
        "ci_width": 6.76
      },
      "biology": {
        "accuracy": 40.4,
        "n": 899,
        "ci_lower": 37.24,
        "ci_upper": 43.64,
        "ci_width": 6.4
      },
      "geography": {
        "accuracy": 23.2,
        "n": 181,
        "ci_lower": 17.65,
        "ci_upper": 29.87,
        "ci_width": 12.22
      },
      "physics": {
        "accuracy": 5.74,
        "n": 540,
        "ci_lower": 4.07,
        "ci_upper": 8.03,
        "ci_width": 3.96
      },
      "politics": {
        "accuracy": 23.45,
        "n": 209,
        "ci_lower": 18.22,
        "ci_upper": 29.64,
        "ci_width": 11.42
      },
      "history": {
        "accuracy": 16.8,
        "n": 256,
        "ci_lower": 12.72,
        "ci_upper": 21.86,
        "ci_width": 9.14
      },
      "administration": {
        "accuracy": 37.27,
        "n": 928,
        "ci_lower": 34.22,
        "ci_upper": 40.43,
        "ci_width": 6.21
      },
      "language": {
        "accuracy": 37.03,
        "n": 740,
        "ci_lower": 33.63,
        "ci_upper": 40.57,
        "ci_width": 6.94
      },
      "medicine": {
        "accuracy": 42.02,
        "n": 238,
        "ci_lower": 35.92,
        "ci_upper": 48.37,
        "ci_width": 12.44
      },
      "engineering": {
        "accuracy": 13.23,
        "n": 688,
        "ci_lower": 10.9,
        "ci_upper": 15.97,
        "ci_width": 5.07
      },
      "computer_science": {
        "accuracy": 33.33,
        "n": 441,
        "ci_lower": 29.09,
        "ci_upper": 37.86,
        "ci_width": 8.77
      },
      "law": {
        "accuracy": 34.36,
        "n": 818,
        "ci_lower": 31.19,
        "ci_upper": 37.68,
        "ci_width": 6.5
      },
      "mathematics": {
        "accuracy": 12.76,
        "n": 1039,
        "ci_lower": 10.87,
        "ci_upper": 14.93,
        "ci_width": 4.06
      }
    }
  },
  {
    "model": "Claude-Sonnet-4",
    "overall": {
      "accuracy": 63.29,
      "n": 8000,
      "ci_lower": 62.23,
      "ci_upper": 64.34,
      "ci_width": 2.11
    },
    "by_nation": {
      "india": {
        "accuracy": 62.51,
        "n": 1699,
        "ci_lower": 60.18,
        "ci_upper": 64.78,
        "ci_width": 4.6
      },
      "eu": {
        "accuracy": 76.43,
        "n": 1289,
        "ci_lower": 74.04,
        "ci_upper": 78.67,
        "ci_width": 4.63
      },
      "taiwan": {
        "accuracy": 87.28,
        "n": 557,
        "ci_lower": 84.26,
        "ci_upper": 89.79,
        "ci_width": 5.54
      },
      "japan": {
        "accuracy": 45.85,
        "n": 2557,
        "ci_lower": 43.93,
        "ci_upper": 47.79,
        "ci_width": 3.86
      },
      "south_korea": {
        "accuracy": 62.41,
        "n": 1898,
        "ci_lower": 60.21,
        "ci_upper": 64.56,
        "ci_width": 4.35
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 67.63,
        "n": 207,
        "ci_lower": 60.98,
        "ci_upper": 73.63,
        "ci_width": 12.65
      },
      "philosophy": {
        "accuracy": 70.87,
        "n": 127,
        "ci_lower": 62.45,
        "ci_upper": 78.07,
        "ci_width": 15.62
      },
      "earth_science": {
        "accuracy": 40.19,
        "n": 107,
        "ci_lower": 31.4,
        "ci_upper": 49.66,
        "ci_width": 18.27
      },
      "psychology": {
        "accuracy": 64.71,
        "n": 119,
        "ci_lower": 55.79,
        "ci_upper": 72.71,
        "ci_width": 16.93
      },
      "economics": {
        "accuracy": 56.83,
        "n": 463,
        "ci_lower": 52.28,
        "ci_upper": 61.27,
        "ci_width": 8.99
      },
      "biology": {
        "accuracy": 68.81,
        "n": 899,
        "ci_lower": 65.71,
        "ci_upper": 71.75,
        "ci_width": 6.05
      },
      "geography": {
        "accuracy": 55.8,
        "n": 181,
        "ci_lower": 48.52,
        "ci_upper": 62.84,
        "ci_width": 14.32
      },
      "physics": {
        "accuracy": 51.67,
        "n": 540,
        "ci_lower": 47.46,
        "ci_upper": 55.86,
        "ci_width": 8.4
      },
      "politics": {
        "accuracy": 63.16,
        "n": 209,
        "ci_lower": 56.44,
        "ci_upper": 69.41,
        "ci_width": 12.97
      },
      "history": {
        "accuracy": 66.02,
        "n": 256,
        "ci_lower": 60.02,
        "ci_upper": 71.55,
        "ci_width": 11.53
      },
      "administration": {
        "accuracy": 64.12,
        "n": 928,
        "ci_lower": 60.98,
        "ci_upper": 67.14,
        "ci_width": 6.16
      },
      "language": {
        "accuracy": 72.3,
        "n": 740,
        "ci_lower": 68.97,
        "ci_upper": 75.4,
        "ci_width": 6.44
      },
      "medicine": {
        "accuracy": 78.99,
        "n": 238,
        "ci_lower": 73.37,
        "ci_upper": 83.68,
        "ci_width": 10.31
      },
      "engineering": {
        "accuracy": 56.4,
        "n": 688,
        "ci_lower": 52.67,
        "ci_upper": 60.06,
        "ci_width": 7.39
      },
      "computer_science": {
        "accuracy": 69.39,
        "n": 441,
        "ci_lower": 64.94,
        "ci_upper": 73.51,
        "ci_width": 8.57
      },
      "law": {
        "accuracy": 60.84,
        "n": 818,
        "ci_lower": 57.45,
        "ci_upper": 64.13,
        "ci_width": 6.68
      },
      "mathematics": {
        "accuracy": 61.61,
        "n": 1039,
        "ci_lower": 58.62,
        "ci_upper": 64.52,
        "ci_width": 5.9
      }
    }
  },
  {
    "model": "Llama-3.2-11B-Vision",
    "overall": {
      "accuracy": 12.75,
      "n": 8000,
      "ci_lower": 12.04,
      "ci_upper": 13.5,
      "ci_width": 1.46
    },
    "by_nation": {
      "india": {
        "accuracy": 13.82,
        "n": 1699,
        "ci_lower": 12.26,
        "ci_upper": 15.54,
        "ci_width": 3.28
      },
      "eu": {
        "accuracy": 20.08,
        "n": 1289,
        "ci_lower": 17.98,
        "ci_upper": 22.35,
        "ci_width": 4.37
      },
      "taiwan": {
        "accuracy": 23.65,
        "n": 557,
        "ci_lower": 20.31,
        "ci_upper": 27.35,
        "ci_width": 7.04
      },
      "japan": {
        "accuracy": 10.06,
        "n": 2557,
        "ci_lower": 8.95,
        "ci_upper": 11.29,
        "ci_width": 2.33
      },
      "south_korea": {
        "accuracy": 6.29,
        "n": 1898,
        "ci_lower": 5.28,
        "ci_upper": 7.47,
        "ci_width": 2.19
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 10.63,
        "n": 207,
        "ci_lower": 7.13,
        "ci_upper": 15.57,
        "ci_width": 8.44
      },
      "philosophy": {
        "accuracy": 7.87,
        "n": 127,
        "ci_lower": 4.33,
        "ci_upper": 13.88,
        "ci_width": 9.55
      },
      "earth_science": {
        "accuracy": 15.89,
        "n": 107,
        "ci_lower": 10.16,
        "ci_upper": 23.98,
        "ci_width": 13.82
      },
      "psychology": {
        "accuracy": 7.56,
        "n": 119,
        "ci_lower": 4.03,
        "ci_upper": 13.75,
        "ci_width": 9.72
      },
      "economics": {
        "accuracy": 10.63,
        "n": 463,
        "ci_lower": 8.14,
        "ci_upper": 13.77,
        "ci_width": 5.63
      },
      "biology": {
        "accuracy": 16.09,
        "n": 899,
        "ci_lower": 13.83,
        "ci_upper": 18.64,
        "ci_width": 4.8
      },
      "geography": {
        "accuracy": 23.2,
        "n": 181,
        "ci_lower": 17.65,
        "ci_upper": 29.87,
        "ci_width": 12.22
      },
      "physics": {
        "accuracy": 11.11,
        "n": 540,
        "ci_lower": 8.73,
        "ci_upper": 14.04,
        "ci_width": 5.31
      },
      "politics": {
        "accuracy": 13.88,
        "n": 209,
        "ci_lower": 9.84,
        "ci_upper": 19.22,
        "ci_width": 9.38
      },
      "history": {
        "accuracy": 11.72,
        "n": 256,
        "ci_lower": 8.33,
        "ci_upper": 16.24,
        "ci_width": 7.9
      },
      "administration": {
        "accuracy": 15.79,
        "n": 928,
        "ci_lower": 13.59,
        "ci_upper": 18.28,
        "ci_width": 4.69
      },
      "language": {
        "accuracy": 13.11,
        "n": 740,
        "ci_lower": 10.87,
        "ci_upper": 15.73,
        "ci_width": 4.87
      },
      "medicine": {
        "accuracy": 13.87,
        "n": 238,
        "ci_lower": 10.05,
        "ci_upper": 18.84,
        "ci_width": 8.79
      },
      "engineering": {
        "accuracy": 8.87,
        "n": 688,
        "ci_lower": 6.97,
        "ci_upper": 11.23,
        "ci_width": 4.26
      },
      "computer_science": {
        "accuracy": 15.42,
        "n": 441,
        "ci_lower": 12.35,
        "ci_upper": 19.09,
        "ci_width": 6.74
      },
      "law": {
        "accuracy": 16.5,
        "n": 818,
        "ci_lower": 14.11,
        "ci_upper": 19.2,
        "ci_width": 5.09
      },
      "mathematics": {
        "accuracy": 6.436,
        "n": 1039,
        "ci_lower": 5.1,
        "ci_upper": 8.09,
        "ci_width": 3.0
      }
    }
  },
  {
    "model": "Qwen2-VL-2B-Instruct",
    "overall": {
      "accuracy": 25.54,
      "n": 8000,
      "ci_lower": 24.6,
      "ci_upper": 26.51,
      "ci_width": 1.91
    },
    "by_nation": {
      "india": {
        "accuracy": 15.09,
        "n": 1699,
        "ci_lower": 13.47,
        "ci_upper": 16.87,
        "ci_width": 3.4
      },
      "eu": {
        "accuracy": 35.9,
        "n": 1289,
        "ci_lower": 33.33,
        "ci_upper": 38.56,
        "ci_width": 5.23
      },
      "taiwan": {
        "accuracy": 33.33,
        "n": 557,
        "ci_lower": 29.54,
        "ci_upper": 37.35,
        "ci_width": 7.81
      },
      "japan": {
        "accuracy": 18.21,
        "n": 2557,
        "ci_lower": 16.76,
        "ci_upper": 19.75,
        "ci_width": 2.99
      },
      "south_korea": {
        "accuracy": 26.13,
        "n": 1898,
        "ci_lower": 24.2,
        "ci_upper": 28.15,
        "ci_width": 3.95
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 24.64,
        "n": 207,
        "ci_lower": 19.27,
        "ci_upper": 30.94,
        "ci_width": 11.67
      },
      "philosophy": {
        "accuracy": 19.68,
        "n": 127,
        "ci_lower": 13.7,
        "ci_upper": 27.44,
        "ci_width": 13.74
      },
      "earth_science": {
        "accuracy": 15.89,
        "n": 107,
        "ci_lower": 10.16,
        "ci_upper": 23.98,
        "ci_width": 13.82
      },
      "psychology": {
        "accuracy": 27.73,
        "n": 119,
        "ci_lower": 20.48,
        "ci_upper": 36.37,
        "ci_width": 15.89
      },
      "economics": {
        "accuracy": 22.99,
        "n": 463,
        "ci_lower": 19.39,
        "ci_upper": 27.04,
        "ci_width": 7.65
      },
      "biology": {
        "accuracy": 27.08,
        "n": 899,
        "ci_lower": 24.28,
        "ci_upper": 30.08,
        "ci_width": 5.8
      },
      "geography": {
        "accuracy": 17.68,
        "n": 181,
        "ci_lower": 12.81,
        "ci_upper": 23.89,
        "ci_width": 11.08
      },
      "physics": {
        "accuracy": 19.63,
        "n": 540,
        "ci_lower": 16.5,
        "ci_upper": 23.19,
        "ci_width": 6.69
      },
      "politics": {
        "accuracy": 22.97,
        "n": 209,
        "ci_lower": 17.79,
        "ci_upper": 29.13,
        "ci_width": 11.34
      },
      "history": {
        "accuracy": 18.36,
        "n": 256,
        "ci_lower": 14.1,
        "ci_upper": 23.56,
        "ci_width": 9.46
      },
      "administration": {
        "accuracy": 33.4,
        "n": 928,
        "ci_lower": 30.44,
        "ci_upper": 36.5,
        "ci_width": 6.06
      },
      "language": {
        "accuracy": 34.32,
        "n": 740,
        "ci_lower": 30.99,
        "ci_upper": 37.81,
        "ci_width": 6.83
      },
      "medicine": {
        "accuracy": 25.63,
        "n": 238,
        "ci_lower": 20.5,
        "ci_upper": 31.53,
        "ci_width": 11.03
      },
      "engineering": {
        "accuracy": 22.97,
        "n": 688,
        "ci_lower": 19.98,
        "ci_upper": 26.26,
        "ci_width": 6.28
      },
      "computer_science": {
        "accuracy": 26.3,
        "n": 441,
        "ci_lower": 22.41,
        "ci_upper": 30.6,
        "ci_width": 8.19
      },
      "law": {
        "accuracy": 28.82,
        "n": 818,
        "ci_lower": 25.82,
        "ci_upper": 32.02,
        "ci_width": 6.2
      },
      "mathematics": {
        "accuracy": 19.19,
        "n": 1039,
        "ci_lower": 16.91,
        "ci_upper": 21.7,
        "ci_width": 4.79
      }
    }
  },
  {
    "model": "Qwen2-VL-7B-Instruct",
    "overall": {
      "accuracy": 31.38,
      "n": 8000,
      "ci_lower": 30.37,
      "ci_upper": 32.41,
      "ci_width": 2.03
    },
    "by_nation": {
      "india": {
        "accuracy": 27.36,
        "n": 1699,
        "ci_lower": 25.29,
        "ci_upper": 29.53,
        "ci_width": 4.24
      },
      "eu": {
        "accuracy": 47.19,
        "n": 1289,
        "ci_lower": 44.48,
        "ci_upper": 49.92,
        "ci_width": 5.44
      },
      "taiwan": {
        "accuracy": 52.51,
        "n": 557,
        "ci_lower": 48.36,
        "ci_upper": 56.63,
        "ci_width": 8.27
      },
      "japan": {
        "accuracy": 15.57,
        "n": 2557,
        "ci_lower": 14.22,
        "ci_upper": 17.03,
        "ci_width": 2.81
      },
      "south_korea": {
        "accuracy": 29.08,
        "n": 1898,
        "ci_lower": 27.08,
        "ci_upper": 31.16,
        "ci_width": 4.08
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 28.5,
        "n": 207,
        "ci_lower": 22.79,
        "ci_upper": 35.0,
        "ci_width": 12.21
      },
      "philosophy": {
        "accuracy": 33.86,
        "n": 127,
        "ci_lower": 26.21,
        "ci_upper": 42.46,
        "ci_width": 16.25
      },
      "earth_science": {
        "accuracy": 12.15,
        "n": 107,
        "ci_lower": 7.24,
        "ci_upper": 19.68,
        "ci_width": 12.44
      },
      "psychology": {
        "accuracy": 26.89,
        "n": 119,
        "ci_lower": 19.74,
        "ci_upper": 35.49,
        "ci_width": 15.75
      },
      "economics": {
        "accuracy": 26.03,
        "n": 463,
        "ci_lower": 22.24,
        "ci_upper": 30.21,
        "ci_width": 7.97
      },
      "biology": {
        "accuracy": 37.51,
        "n": 899,
        "ci_lower": 34.4,
        "ci_upper": 40.72,
        "ci_width": 6.32
      },
      "geography": {
        "accuracy": 30.39,
        "n": 181,
        "ci_lower": 24.15,
        "ci_upper": 37.44,
        "ci_width": 13.29
      },
      "physics": {
        "accuracy": 16.11,
        "n": 540,
        "ci_lower": 13.25,
        "ci_upper": 19.45,
        "ci_width": 6.2
      },
      "politics": {
        "accuracy": 29.67,
        "n": 209,
        "ci_lower": 23.89,
        "ci_upper": 36.18,
        "ci_width": 12.3
      },
      "history": {
        "accuracy": 25.39,
        "n": 256,
        "ci_lower": 20.45,
        "ci_upper": 31.06,
        "ci_width": 10.61
      },
      "administration": {
        "accuracy": 41.78,
        "n": 928,
        "ci_lower": 38.65,
        "ci_upper": 44.98,
        "ci_width": 6.33
      },
      "language": {
        "accuracy": 41.89,
        "n": 740,
        "ci_lower": 38.39,
        "ci_upper": 45.48,
        "ci_width": 7.09
      },
      "medicine": {
        "accuracy": 37.82,
        "n": 238,
        "ci_lower": 31.9,
        "ci_upper": 44.13,
        "ci_width": 12.23
      },
      "engineering": {
        "accuracy": 20.93,
        "n": 688,
        "ci_lower": 18.06,
        "ci_upper": 24.13,
        "ci_width": 6.07
      },
      "computer_science": {
        "accuracy": 41.04,
        "n": 441,
        "ci_lower": 36.55,
        "ci_upper": 45.69,
        "ci_width": 9.14
      },
      "law": {
        "accuracy": 37.81,
        "n": 818,
        "ci_lower": 34.55,
        "ci_upper": 41.18,
        "ci_width": 6.63
      },
      "mathematics": {
        "accuracy": 20.73,
        "n": 1039,
        "ci_lower": 18.38,
        "ci_upper": 23.3,
        "ci_width": 4.93
      }
    }
  },
  {
    "model": "Qwen2.5-VL-7B-Instruct",
    "overall": {
      "accuracy": 32.3,
      "n": 8000,
      "ci_lower": 31.28,
      "ci_upper": 33.33,
      "ci_width": 2.05
    },
    "by_nation": {
      "india": {
        "accuracy": 26.29,
        "n": 1699,
        "ci_lower": 24.25,
        "ci_upper": 28.44,
        "ci_width": 4.18
      },
      "eu": {
        "accuracy": 45.89,
        "n": 1289,
        "ci_lower": 43.19,
        "ci_upper": 48.62,
        "ci_width": 5.43
      },
      "taiwan": {
        "accuracy": 46.95,
        "n": 557,
        "ci_lower": 42.84,
        "ci_upper": 51.1,
        "ci_width": 8.26
      },
      "japan": {
        "accuracy": 21.88,
        "n": 2557,
        "ci_lower": 20.32,
        "ci_upper": 23.52,
        "ci_width": 3.2
      },
      "south_korea": {
        "accuracy": 29.53,
        "n": 1898,
        "ci_lower": 27.52,
        "ci_upper": 31.62,
        "ci_width": 4.1
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 33.33,
        "n": 207,
        "ci_lower": 27.26,
        "ci_upper": 40.0,
        "ci_width": 12.74
      },
      "philosophy": {
        "accuracy": 30.71,
        "n": 127,
        "ci_lower": 23.35,
        "ci_upper": 39.2,
        "ci_width": 15.85
      },
      "earth_science": {
        "accuracy": 18.69,
        "n": 107,
        "ci_lower": 12.44,
        "ci_upper": 27.11,
        "ci_width": 14.68
      },
      "psychology": {
        "accuracy": 25.21,
        "n": 119,
        "ci_lower": 18.27,
        "ci_upper": 33.7,
        "ci_width": 15.44
      },
      "economics": {
        "accuracy": 25.16,
        "n": 463,
        "ci_lower": 21.42,
        "ci_upper": 29.31,
        "ci_width": 7.88
      },
      "biology": {
        "accuracy": 35.18,
        "n": 899,
        "ci_lower": 32.13,
        "ci_upper": 38.36,
        "ci_width": 6.23
      },
      "geography": {
        "accuracy": 30.39,
        "n": 181,
        "ci_lower": 24.15,
        "ci_upper": 37.44,
        "ci_width": 13.29
      },
      "physics": {
        "accuracy": 25.0,
        "n": 540,
        "ci_lower": 21.53,
        "ci_upper": 28.82,
        "ci_width": 7.29
      },
      "politics": {
        "accuracy": 24.4,
        "n": 209,
        "ci_lower": 19.07,
        "ci_upper": 30.65,
        "ci_width": 11.58
      },
      "history": {
        "accuracy": 26.95,
        "n": 256,
        "ci_lower": 21.89,
        "ci_upper": 32.7,
        "ci_width": 10.81
      },
      "administration": {
        "accuracy": 36.52,
        "n": 928,
        "ci_lower": 33.48,
        "ci_upper": 39.67,
        "ci_width": 6.18
      },
      "language": {
        "accuracy": 43.51,
        "n": 740,
        "ci_lower": 39.98,
        "ci_upper": 47.11,
        "ci_width": 7.13
      },
      "medicine": {
        "accuracy": 36.13,
        "n": 238,
        "ci_lower": 30.29,
        "ci_upper": 42.41,
        "ci_width": 12.12
      },
      "engineering": {
        "accuracy": 26.02,
        "n": 688,
        "ci_lower": 22.88,
        "ci_upper": 29.43,
        "ci_width": 6.54
      },
      "computer_science": {
        "accuracy": 41.72,
        "n": 441,
        "ci_lower": 37.21,
        "ci_upper": 46.37,
        "ci_width": 9.17
      },
      "law": {
        "accuracy": 34.48,
        "n": 818,
        "ci_lower": 31.3,
        "ci_upper": 37.8,
        "ci_width": 6.5
      },
      "mathematics": {
        "accuracy": 28.02,
        "n": 1039,
        "ci_lower": 25.37,
        "ci_upper": 30.83,
        "ci_width": 5.45
      }
    }
  },
  {
    "model": "Phi-3.5-vision-instruct",
    "overall": {
      "accuracy": 15.67,
      "n": 8000,
      "ci_lower": 14.89,
      "ci_upper": 16.48,
      "ci_width": 1.59
    },
    "by_nation": {
      "india": {
        "accuracy": 15.48,
        "n": 1699,
        "ci_lower": 13.84,
        "ci_upper": 17.28,
        "ci_width": 3.44
      },
      "eu": {
        "accuracy": 19.56,
        "n": 1289,
        "ci_lower": 17.49,
        "ci_upper": 21.81,
        "ci_width": 4.33
      },
      "taiwan": {
        "accuracy": 15.77,
        "n": 557,
        "ci_lower": 12.98,
        "ci_upper": 19.03,
        "ci_width": 6.05
      },
      "japan": {
        "accuracy": 14.64,
        "n": 2557,
        "ci_lower": 13.32,
        "ci_upper": 16.06,
        "ci_width": 2.74
      },
      "south_korea": {
        "accuracy": 13.53,
        "n": 1898,
        "ci_lower": 12.06,
        "ci_upper": 15.14,
        "ci_width": 3.08
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 18.36,
        "n": 207,
        "ci_lower": 13.68,
        "ci_upper": 24.19,
        "ci_width": 10.52
      },
      "philosophy": {
        "accuracy": 11.02,
        "n": 127,
        "ci_lower": 6.68,
        "ci_upper": 17.65,
        "ci_width": 10.97
      },
      "earth_science": {
        "accuracy": 10.28,
        "n": 107,
        "ci_lower": 5.84,
        "ci_upper": 17.48,
        "ci_width": 11.64
      },
      "psychology": {
        "accuracy": 16.81,
        "n": 119,
        "ci_lower": 11.15,
        "ci_upper": 24.54,
        "ci_width": 13.39
      },
      "economics": {
        "accuracy": 14.32,
        "n": 463,
        "ci_lower": 11.42,
        "ci_upper": 17.8,
        "ci_width": 6.38
      },
      "biology": {
        "accuracy": 14.21,
        "n": 899,
        "ci_lower": 12.08,
        "ci_upper": 16.64,
        "ci_width": 4.57
      },
      "geography": {
        "accuracy": 19.34,
        "n": 181,
        "ci_lower": 14.25,
        "ci_upper": 25.71,
        "ci_width": 11.46
      },
      "physics": {
        "accuracy": 13.52,
        "n": 540,
        "ci_lower": 10.89,
        "ci_upper": 16.66,
        "ci_width": 5.77
      },
      "politics": {
        "accuracy": 17.7,
        "n": 209,
        "ci_lower": 13.12,
        "ci_upper": 23.44,
        "ci_width": 10.32
      },
      "history": {
        "accuracy": 16.8,
        "n": 256,
        "ci_lower": 12.72,
        "ci_upper": 21.86,
        "ci_width": 9.14
      },
      "administration": {
        "accuracy": 19.01,
        "n": 928,
        "ci_lower": 16.62,
        "ci_upper": 21.66,
        "ci_width": 5.05
      },
      "language": {
        "accuracy": 15.54,
        "n": 740,
        "ci_lower": 13.11,
        "ci_upper": 18.33,
        "ci_width": 5.22
      },
      "medicine": {
        "accuracy": 13.45,
        "n": 238,
        "ci_lower": 9.69,
        "ci_upper": 18.37,
        "ci_width": 8.68
      },
      "engineering": {
        "accuracy": 15.41,
        "n": 688,
        "ci_lower": 12.9,
        "ci_upper": 18.3,
        "ci_width": 5.39
      },
      "computer_science": {
        "accuracy": 16.55,
        "n": 441,
        "ci_lower": 13.37,
        "ci_upper": 20.3,
        "ci_width": 6.93
      },
      "law": {
        "accuracy": 18.6,
        "n": 818,
        "ci_lower": 16.08,
        "ci_upper": 21.41,
        "ci_width": 5.33
      },
      "mathematics": {
        "accuracy": 12.96,
        "n": 1039,
        "ci_lower": 11.05,
        "ci_upper": 15.14,
        "ci_width": 4.09
      }
    }
  },
  {
    "model": "Qwen2-VL-72B-Instruct",
    "overall": {
      "accuracy": 44.65,
      "n": 8000,
      "ci_lower": 43.56,
      "ci_upper": 45.74,
      "ci_width": 2.18
    },
    "by_nation": {
      "india": {
        "accuracy": 35.93,
        "n": 1699,
        "ci_lower": 33.68,
        "ci_upper": 38.24,
        "ci_width": 4.56
      },
      "eu": {
        "accuracy": 62.07,
        "n": 1289,
        "ci_lower": 59.39,
        "ci_upper": 64.68,
        "ci_width": 5.29
      },
      "taiwan": {
        "accuracy": 74.73,
        "n": 557,
        "ci_lower": 70.96,
        "ci_upper": 78.16,
        "ci_width": 7.2
      },
      "japan": {
        "accuracy": 30.37,
        "n": 2557,
        "ci_lower": 28.62,
        "ci_upper": 32.18,
        "ci_width": 3.56
      },
      "south_korea": {
        "accuracy": 39.71,
        "n": 1898,
        "ci_lower": 37.53,
        "ci_upper": 41.93,
        "ci_width": 4.4
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 52.66,
        "n": 207,
        "ci_lower": 45.87,
        "ci_upper": 59.35,
        "ci_width": 13.48
      },
      "philosophy": {
        "accuracy": 41.73,
        "n": 127,
        "ci_lower": 33.52,
        "ci_upper": 50.43,
        "ci_width": 16.91
      },
      "earth_science": {
        "accuracy": 26.17,
        "n": 107,
        "ci_lower": 18.77,
        "ci_upper": 35.22,
        "ci_width": 16.45
      },
      "psychology": {
        "accuracy": 38.66,
        "n": 119,
        "ci_lower": 30.4,
        "ci_upper": 47.63,
        "ci_width": 17.24
      },
      "economics": {
        "accuracy": 34.27,
        "n": 463,
        "ci_lower": 30.09,
        "ci_upper": 38.71,
        "ci_width": 8.61
      },
      "biology": {
        "accuracy": 48.72,
        "n": 899,
        "ci_lower": 45.46,
        "ci_upper": 51.99,
        "ci_width": 6.52
      },
      "geography": {
        "accuracy": 37.57,
        "n": 181,
        "ci_lower": 30.84,
        "ci_upper": 44.82,
        "ci_width": 13.97
      },
      "physics": {
        "accuracy": 37.41,
        "n": 540,
        "ci_lower": 33.43,
        "ci_upper": 41.57,
        "ci_width": 8.14
      },
      "politics": {
        "accuracy": 35.41,
        "n": 209,
        "ci_lower": 29.24,
        "ci_upper": 42.1,
        "ci_width": 12.86
      },
      "history": {
        "accuracy": 31.25,
        "n": 256,
        "ci_lower": 25.88,
        "ci_upper": 37.17,
        "ci_width": 11.29
      },
      "administration": {
        "accuracy": 51.99,
        "n": 928,
        "ci_lower": 48.77,
        "ci_upper": 55.19,
        "ci_width": 6.42
      },
      "language": {
        "accuracy": 53.38,
        "n": 740,
        "ci_lower": 49.78,
        "ci_upper": 56.95,
        "ci_width": 7.17
      },
      "medicine": {
        "accuracy": 50.84,
        "n": 238,
        "ci_lower": 44.53,
        "ci_upper": 57.13,
        "ci_width": 12.6
      },
      "engineering": {
        "accuracy": 35.61,
        "n": 688,
        "ci_lower": 32.12,
        "ci_upper": 39.26,
        "ci_width": 7.14
      },
      "computer_science": {
        "accuracy": 58.28,
        "n": 441,
        "ci_lower": 53.63,
        "ci_upper": 62.79,
        "ci_width": 9.17
      },
      "law": {
        "accuracy": 48.03,
        "n": 818,
        "ci_lower": 44.62,
        "ci_upper": 51.46,
        "ci_width": 6.83
      },
      "mathematics": {
        "accuracy": 40.6,
        "n": 1039,
        "ci_lower": 37.65,
        "ci_upper": 43.62,
        "ci_width": 5.96
      }
    }
  },
  {
    "model": "InternVL2.5-38B-MPO",
    "overall": {
      "accuracy": 39.34,
      "n": 8000,
      "ci_lower": 38.27,
      "ci_upper": 40.42,
      "ci_width": 2.14
    },
    "by_nation": {
      "india": {
        "accuracy": 19.38,
        "n": 1699,
        "ci_lower": 17.57,
        "ci_upper": 21.33,
        "ci_width": 3.76
      },
      "eu": {
        "accuracy": 52.91,
        "n": 1289,
        "ci_lower": 50.18,
        "ci_upper": 55.62,
        "ci_width": 5.44
      },
      "taiwan": {
        "accuracy": 56.81,
        "n": 557,
        "ci_lower": 52.66,
        "ci_upper": 60.86,
        "ci_width": 8.2
      },
      "japan": {
        "accuracy": 31.49,
        "n": 2557,
        "ci_lower": 29.72,
        "ci_upper": 33.32,
        "ci_width": 3.6
      },
      "south_korea": {
        "accuracy": 39.63,
        "n": 1898,
        "ci_lower": 37.45,
        "ci_upper": 41.85,
        "ci_width": 4.4
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 38.16,
        "n": 207,
        "ci_lower": 31.82,
        "ci_upper": 44.94,
        "ci_width": 13.12
      },
      "philosophy": {
        "accuracy": 30.71,
        "n": 127,
        "ci_lower": 23.35,
        "ci_upper": 39.2,
        "ci_width": 15.85
      },
      "earth_science": {
        "accuracy": 14.02,
        "n": 107,
        "ci_lower": 8.68,
        "ci_upper": 21.85,
        "ci_width": 13.17
      },
      "psychology": {
        "accuracy": 34.45,
        "n": 119,
        "ci_lower": 26.52,
        "ci_upper": 43.35,
        "ci_width": 16.84
      },
      "economics": {
        "accuracy": 27.12,
        "n": 463,
        "ci_lower": 23.27,
        "ci_upper": 31.35,
        "ci_width": 8.07
      },
      "biology": {
        "accuracy": 41.18,
        "n": 899,
        "ci_lower": 38.01,
        "ci_upper": 44.43,
        "ci_width": 6.42
      },
      "geography": {
        "accuracy": 24.86,
        "n": 181,
        "ci_lower": 19.13,
        "ci_upper": 31.64,
        "ci_width": 12.51
      },
      "physics": {
        "accuracy": 36.48,
        "n": 540,
        "ci_lower": 32.53,
        "ci_upper": 40.62,
        "ci_width": 8.09
      },
      "politics": {
        "accuracy": 35.89,
        "n": 209,
        "ci_lower": 29.7,
        "ci_upper": 42.59,
        "ci_width": 12.9
      },
      "history": {
        "accuracy": 26.56,
        "n": 256,
        "ci_lower": 21.53,
        "ci_upper": 32.29,
        "ci_width": 10.76
      },
      "administration": {
        "accuracy": 44.04,
        "n": 928,
        "ci_lower": 40.88,
        "ci_upper": 47.25,
        "ci_width": 6.38
      },
      "language": {
        "accuracy": 49.73,
        "n": 740,
        "ci_lower": 46.14,
        "ci_upper": 53.32,
        "ci_width": 7.19
      },
      "medicine": {
        "accuracy": 39.5,
        "n": 238,
        "ci_lower": 33.5,
        "ci_upper": 45.83,
        "ci_width": 12.33
      },
      "engineering": {
        "accuracy": 34.74,
        "n": 688,
        "ci_lower": 31.28,
        "ci_upper": 38.37,
        "ci_width": 7.1
      },
      "computer_science": {
        "accuracy": 56.24,
        "n": 441,
        "ci_lower": 51.58,
        "ci_upper": 60.8,
        "ci_width": 9.22
      },
      "law": {
        "accuracy": 40.02,
        "n": 818,
        "ci_lower": 36.72,
        "ci_upper": 43.42,
        "ci_width": 6.7
      },
      "mathematics": {
        "accuracy": 39.16,
        "n": 1039,
        "ci_lower": 36.24,
        "ci_upper": 42.16,
        "ci_width": 5.93
      }
    }
  },
  {
    "model": "Ovis2-8B",
    "overall": {
      "accuracy": 28.31,
      "n": 8000,
      "ci_lower": 27.33,
      "ci_upper": 29.31,
      "ci_width": 1.97
    },
    "by_nation": {
      "india": {
        "accuracy": 25.02,
        "n": 1699,
        "ci_lower": 23.02,
        "ci_upper": 27.13,
        "ci_width": 4.12
      },
      "eu": {
        "accuracy": 40.63,
        "n": 1289,
        "ci_lower": 37.98,
        "ci_upper": 43.34,
        "ci_width": 5.35
      },
      "taiwan": {
        "accuracy": 34.77,
        "n": 557,
        "ci_lower": 30.93,
        "ci_upper": 38.82,
        "ci_width": 7.89
      },
      "japan": {
        "accuracy": 17.58,
        "n": 2557,
        "ci_lower": 16.15,
        "ci_upper": 19.1,
        "ci_width": 2.95
      },
      "south_korea": {
        "accuracy": 27.53,
        "n": 1898,
        "ci_lower": 25.57,
        "ci_upper": 29.58,
        "ci_width": 4.02
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 30.43,
        "n": 207,
        "ci_lower": 24.57,
        "ci_upper": 37.01,
        "ci_width": 12.44
      },
      "philosophy": {
        "accuracy": 31.5,
        "n": 127,
        "ci_lower": 24.07,
        "ci_upper": 40.02,
        "ci_width": 15.96
      },
      "earth_science": {
        "accuracy": 17.76,
        "n": 107,
        "ci_lower": 11.68,
        "ci_upper": 26.08,
        "ci_width": 14.4
      },
      "psychology": {
        "accuracy": 21.85,
        "n": 119,
        "ci_lower": 15.37,
        "ci_upper": 30.09,
        "ci_width": 14.72
      },
      "economics": {
        "accuracy": 23.64,
        "n": 463,
        "ci_lower": 20.0,
        "ci_upper": 27.72,
        "ci_width": 7.72
      },
      "biology": {
        "accuracy": 29.97,
        "n": 899,
        "ci_lower": 27.07,
        "ci_upper": 33.04,
        "ci_width": 5.98
      },
      "geography": {
        "accuracy": 29.83,
        "n": 181,
        "ci_lower": 23.64,
        "ci_upper": 36.86,
        "ci_width": 13.22
      },
      "physics": {
        "accuracy": 17.41,
        "n": 540,
        "ci_lower": 14.44,
        "ci_upper": 20.84,
        "ci_width": 6.39
      },
      "politics": {
        "accuracy": 25.84,
        "n": 209,
        "ci_lower": 20.38,
        "ci_upper": 32.17,
        "ci_width": 11.79
      },
      "history": {
        "accuracy": 24.22,
        "n": 256,
        "ci_lower": 19.38,
        "ci_upper": 29.82,
        "ci_width": 10.45
      },
      "administration": {
        "accuracy": 36.09,
        "n": 928,
        "ci_lower": 33.06,
        "ci_upper": 39.23,
        "ci_width": 6.17
      },
      "language": {
        "accuracy": 37.3,
        "n": 740,
        "ci_lower": 33.89,
        "ci_upper": 40.84,
        "ci_width": 6.95
      },
      "medicine": {
        "accuracy": 26.05,
        "n": 238,
        "ci_lower": 20.89,
        "ci_upper": 31.98,
        "ci_width": 11.09
      },
      "engineering": {
        "accuracy": 21.51,
        "n": 688,
        "ci_lower": 18.6,
        "ci_upper": 24.73,
        "ci_width": 6.13
      },
      "computer_science": {
        "accuracy": 34.92,
        "n": 441,
        "ci_lower": 30.62,
        "ci_upper": 39.48,
        "ci_width": 8.86
      },
      "law": {
        "accuracy": 31.16,
        "n": 818,
        "ci_lower": 28.08,
        "ci_upper": 34.42,
        "ci_width": 6.34
      },
      "mathematics": {
        "accuracy": 23.51,
        "n": 1039,
        "ci_lower": 21.03,
        "ci_upper": 26.18,
        "ci_width": 5.15
      }
    }
  },
  {
    "model": "Ovis2-16B",
    "overall": {
      "accuracy": 32.73,
      "n": 8000,
      "ci_lower": 31.71,
      "ci_upper": 33.77,
      "ci_width": 2.06
    },
    "by_nation": {
      "india": {
        "accuracy": 20.06,
        "n": 1699,
        "ci_lower": 18.22,
        "ci_upper": 22.03,
        "ci_width": 3.81
      },
      "eu": {
        "accuracy": 45.32,
        "n": 1289,
        "ci_lower": 42.62,
        "ci_upper": 48.05,
        "ci_width": 5.43
      },
      "taiwan": {
        "accuracy": 51.61,
        "n": 557,
        "ci_lower": 47.46,
        "ci_upper": 55.73,
        "ci_width": 8.27
      },
      "japan": {
        "accuracy": 26.71,
        "n": 2557,
        "ci_lower": 25.03,
        "ci_upper": 28.46,
        "ci_width": 3.43
      },
      "south_korea": {
        "accuracy": 28.88,
        "n": 1898,
        "ci_lower": 26.89,
        "ci_upper": 30.96,
        "ci_width": 4.07
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 33.82,
        "n": 207,
        "ci_lower": 27.72,
        "ci_upper": 40.51,
        "ci_width": 12.79
      },
      "philosophy": {
        "accuracy": 25.98,
        "n": 127,
        "ci_lower": 19.14,
        "ci_upper": 34.23,
        "ci_width": 15.09
      },
      "earth_science": {
        "accuracy": 19.63,
        "n": 107,
        "ci_lower": 13.21,
        "ci_upper": 28.15,
        "ci_width": 14.94
      },
      "psychology": {
        "accuracy": 21.85,
        "n": 119,
        "ci_lower": 15.37,
        "ci_upper": 30.09,
        "ci_width": 14.72
      },
      "economics": {
        "accuracy": 22.99,
        "n": 463,
        "ci_lower": 19.39,
        "ci_upper": 27.04,
        "ci_width": 7.65
      },
      "biology": {
        "accuracy": 35.29,
        "n": 899,
        "ci_lower": 32.23,
        "ci_upper": 38.47,
        "ci_width": 6.24
      },
      "geography": {
        "accuracy": 25.41,
        "n": 181,
        "ci_lower": 19.62,
        "ci_upper": 32.22,
        "ci_width": 12.59
      },
      "physics": {
        "accuracy": 32.59,
        "n": 540,
        "ci_lower": 28.77,
        "ci_upper": 36.65,
        "ci_width": 7.88
      },
      "politics": {
        "accuracy": 29.67,
        "n": 209,
        "ci_lower": 23.89,
        "ci_upper": 36.18,
        "ci_width": 12.3
      },
      "history": {
        "accuracy": 24.22,
        "n": 256,
        "ci_lower": 19.38,
        "ci_upper": 29.82,
        "ci_width": 10.45
      },
      "administration": {
        "accuracy": 39.21,
        "n": 928,
        "ci_lower": 36.12,
        "ci_upper": 42.39,
        "ci_width": 6.27
      },
      "language": {
        "accuracy": 42.97,
        "n": 740,
        "ci_lower": 39.45,
        "ci_upper": 46.56,
        "ci_width": 7.12
      },
      "medicine": {
        "accuracy": 35.29,
        "n": 238,
        "ci_lower": 29.5,
        "ci_upper": 41.55,
        "ci_width": 12.05
      },
      "engineering": {
        "accuracy": 26.16,
        "n": 688,
        "ci_lower": 23.01,
        "ci_upper": 29.57,
        "ci_width": 6.56
      },
      "computer_science": {
        "accuracy": 38.78,
        "n": 441,
        "ci_lower": 34.35,
        "ci_upper": 43.41,
        "ci_width": 9.06
      },
      "law": {
        "accuracy": 30.05,
        "n": 818,
        "ci_lower": 27.01,
        "ci_upper": 33.28,
        "ci_width": 6.27
      },
      "mathematics": {
        "accuracy": 32.25,
        "n": 1039,
        "ci_lower": 29.48,
        "ci_upper": 35.15,
        "ci_width": 5.68
      }
    }
  },
  {
    "model": "Ovis2-32B",
    "overall": {
      "accuracy": 35.5,
      "n": 8000,
      "ci_lower": 34.46,
      "ci_upper": 36.56,
      "ci_width": 2.1
    },
    "by_nation": {
      "india": {
        "accuracy": 22.49,
        "n": 1699,
        "ci_lower": 20.57,
        "ci_upper": 24.54,
        "ci_width": 3.97
      },
      "eu": {
        "accuracy": 51.61,
        "n": 1289,
        "ci_lower": 48.88,
        "ci_upper": 54.33,
        "ci_width": 5.45
      },
      "taiwan": {
        "accuracy": 54.48,
        "n": 557,
        "ci_lower": 50.33,
        "ci_upper": 58.57,
        "ci_width": 8.24
      },
      "japan": {
        "accuracy": 28.42,
        "n": 2557,
        "ci_lower": 26.71,
        "ci_upper": 30.2,
        "ci_width": 3.49
      },
      "south_korea": {
        "accuracy": 29.9,
        "n": 1898,
        "ci_lower": 27.88,
        "ci_upper": 32.0,
        "ci_width": 4.12
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 32.37,
        "n": 207,
        "ci_lower": 26.37,
        "ci_upper": 39.02,
        "ci_width": 12.65
      },
      "philosophy": {
        "accuracy": 30.71,
        "n": 127,
        "ci_lower": 23.35,
        "ci_upper": 39.2,
        "ci_width": 15.85
      },
      "earth_science": {
        "accuracy": 15.89,
        "n": 107,
        "ci_lower": 10.16,
        "ci_upper": 23.98,
        "ci_width": 13.82
      },
      "psychology": {
        "accuracy": 23.53,
        "n": 119,
        "ci_lower": 16.81,
        "ci_upper": 31.9,
        "ci_width": 15.09
      },
      "economics": {
        "accuracy": 25.6,
        "n": 463,
        "ci_lower": 21.84,
        "ci_upper": 29.76,
        "ci_width": 7.93
      },
      "biology": {
        "accuracy": 37.62,
        "n": 899,
        "ci_lower": 34.51,
        "ci_upper": 40.83,
        "ci_width": 6.32
      },
      "geography": {
        "accuracy": 27.62,
        "n": 181,
        "ci_lower": 21.62,
        "ci_upper": 34.55,
        "ci_width": 12.93
      },
      "physics": {
        "accuracy": 36.11,
        "n": 540,
        "ci_lower": 32.17,
        "ci_upper": 40.25,
        "ci_width": 8.08
      },
      "politics": {
        "accuracy": 30.14,
        "n": 209,
        "ci_lower": 24.32,
        "ci_upper": 36.67,
        "ci_width": 12.35
      },
      "history": {
        "accuracy": 18.75,
        "n": 256,
        "ci_lower": 14.44,
        "ci_upper": 23.98,
        "ci_width": 9.54
      },
      "administration": {
        "accuracy": 41.25,
        "n": 928,
        "ci_lower": 38.13,
        "ci_upper": 44.45,
        "ci_width": 6.32
      },
      "language": {
        "accuracy": 45.95,
        "n": 740,
        "ci_lower": 42.39,
        "ci_upper": 49.55,
        "ci_width": 7.16
      },
      "medicine": {
        "accuracy": 40.34,
        "n": 238,
        "ci_lower": 34.31,
        "ci_upper": 46.68,
        "ci_width": 12.37
      },
      "engineering": {
        "accuracy": 24.71,
        "n": 688,
        "ci_lower": 21.63,
        "ci_upper": 28.07,
        "ci_width": 6.43
      },
      "computer_science": {
        "accuracy": 43.99,
        "n": 441,
        "ci_lower": 39.43,
        "ci_upper": 48.65,
        "ci_width": 9.23
      },
      "law": {
        "accuracy": 38.05,
        "n": 818,
        "ci_lower": 34.79,
        "ci_upper": 41.43,
        "ci_width": 6.64
      },
      "mathematics": {
        "accuracy": 36.76,
        "n": 1039,
        "ci_lower": 33.88,
        "ci_upper": 39.74,
        "ci_width": 5.85
      }
    }
  },
  {
    "model": "llama3-llava-next-8b",
    "overall": {
      "accuracy": 14.28,
      "n": 8000,
      "ci_lower": 13.53,
      "ci_upper": 15.06,
      "ci_width": 1.53
    },
    "by_nation": {
      "india": {
        "accuracy": 7.89,
        "n": 1699,
        "ci_lower": 6.7,
        "ci_upper": 9.27,
        "ci_width": 2.57
      },
      "eu": {
        "accuracy": 16.65,
        "n": 1289,
        "ci_lower": 14.72,
        "ci_upper": 18.78,
        "ci_width": 4.07
      },
      "taiwan": {
        "accuracy": 17.92,
        "n": 557,
        "ci_lower": 14.96,
        "ci_upper": 21.32,
        "ci_width": 6.36
      },
      "japan": {
        "accuracy": 12.3,
        "n": 2557,
        "ci_lower": 11.08,
        "ci_upper": 13.63,
        "ci_width": 2.55
      },
      "south_korea": {
        "accuracy": 15.91,
        "n": 1898,
        "ci_lower": 14.33,
        "ci_upper": 17.62,
        "ci_width": 3.29
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 20.77,
        "n": 207,
        "ci_lower": 15.8,
        "ci_upper": 26.8,
        "ci_width": 11.0
      },
      "philosophy": {
        "accuracy": 19.69,
        "n": 127,
        "ci_lower": 13.71,
        "ci_upper": 27.45,
        "ci_width": 13.74
      },
      "earth_science": {
        "accuracy": 7.48,
        "n": 107,
        "ci_lower": 3.84,
        "ci_upper": 14.07,
        "ci_width": 10.23
      },
      "psychology": {
        "accuracy": 15.97,
        "n": 119,
        "ci_lower": 10.47,
        "ci_upper": 23.6,
        "ci_width": 13.13
      },
      "economics": {
        "accuracy": 12.8,
        "n": 463,
        "ci_lower": 10.06,
        "ci_upper": 16.15,
        "ci_width": 6.09
      },
      "biology": {
        "accuracy": 14.21,
        "n": 899,
        "ci_lower": 12.08,
        "ci_upper": 16.64,
        "ci_width": 4.57
      },
      "geography": {
        "accuracy": 12.71,
        "n": 181,
        "ci_lower": 8.62,
        "ci_upper": 18.35,
        "ci_width": 9.73
      },
      "physics": {
        "accuracy": 14.07,
        "n": 540,
        "ci_lower": 11.39,
        "ci_upper": 17.26,
        "ci_width": 5.87
      },
      "politics": {
        "accuracy": 9.09,
        "n": 209,
        "ci_lower": 5.9,
        "ci_upper": 13.76,
        "ci_width": 7.86
      },
      "history": {
        "accuracy": 15.63,
        "n": 256,
        "ci_lower": 11.69,
        "ci_upper": 20.58,
        "ci_width": 8.89
      },
      "administration": {
        "accuracy": 17.29,
        "n": 928,
        "ci_lower": 14.99,
        "ci_upper": 19.86,
        "ci_width": 4.86
      },
      "language": {
        "accuracy": 14.86,
        "n": 740,
        "ci_lower": 12.48,
        "ci_upper": 17.6,
        "ci_width": 5.13
      },
      "medicine": {
        "accuracy": 16.81,
        "n": 238,
        "ci_lower": 12.59,
        "ci_upper": 22.08,
        "ci_width": 9.49
      },
      "engineering": {
        "accuracy": 11.77,
        "n": 688,
        "ci_lower": 9.57,
        "ci_upper": 14.39,
        "ci_width": 4.82
      },
      "computer_science": {
        "accuracy": 12.93,
        "n": 441,
        "ci_lower": 10.12,
        "ci_upper": 16.38,
        "ci_width": 6.27
      },
      "law": {
        "accuracy": 16.01,
        "n": 818,
        "ci_lower": 13.66,
        "ci_upper": 18.68,
        "ci_width": 5.02
      },
      "mathematics": {
        "accuracy": 11.8,
        "n": 1039,
        "ci_lower": 9.98,
        "ci_upper": 13.9,
        "ci_width": 3.93
      }
    }
  },
  {
    "model": "llava-1.5-13b",
    "overall": {
      "accuracy": 18.99,
      "n": 8000,
      "ci_lower": 18.15,
      "ci_upper": 19.86,
      "ci_width": 1.72
    },
    "by_nation": {
      "india": {
        "accuracy": 16.65,
        "n": 1699,
        "ci_lower": 14.95,
        "ci_upper": 18.5,
        "ci_width": 3.54
      },
      "eu": {
        "accuracy": 21.59,
        "n": 1289,
        "ci_lower": 19.43,
        "ci_upper": 23.92,
        "ci_width": 4.49
      },
      "taiwan": {
        "accuracy": 17.92,
        "n": 557,
        "ci_lower": 14.96,
        "ci_upper": 21.32,
        "ci_width": 6.36
      },
      "japan": {
        "accuracy": 19.48,
        "n": 2557,
        "ci_lower": 17.99,
        "ci_upper": 21.06,
        "ci_width": 3.07
      },
      "south_korea": {
        "accuracy": 17.75,
        "n": 1898,
        "ci_lower": 16.1,
        "ci_upper": 19.53,
        "ci_width": 3.44
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 15.94,
        "n": 207,
        "ci_lower": 11.58,
        "ci_upper": 21.54,
        "ci_width": 9.96
      },
      "philosophy": {
        "accuracy": 16.54,
        "n": 127,
        "ci_lower": 11.08,
        "ci_upper": 23.96,
        "ci_width": 12.88
      },
      "earth_science": {
        "accuracy": 18.69,
        "n": 107,
        "ci_lower": 12.44,
        "ci_upper": 27.11,
        "ci_width": 14.68
      },
      "psychology": {
        "accuracy": 19.33,
        "n": 119,
        "ci_lower": 13.24,
        "ci_upper": 27.34,
        "ci_width": 14.1
      },
      "economics": {
        "accuracy": 19.52,
        "n": 463,
        "ci_lower": 16.17,
        "ci_upper": 23.38,
        "ci_width": 7.21
      },
      "biology": {
        "accuracy": 17.31,
        "n": 899,
        "ci_lower": 14.98,
        "ci_upper": 19.92,
        "ci_width": 4.94
      },
      "geography": {
        "accuracy": 23.76,
        "n": 181,
        "ci_lower": 18.15,
        "ci_upper": 30.47,
        "ci_width": 12.32
      },
      "physics": {
        "accuracy": 19.44,
        "n": 540,
        "ci_lower": 16.32,
        "ci_upper": 22.99,
        "ci_width": 6.67
      },
      "politics": {
        "accuracy": 19.62,
        "n": 209,
        "ci_lower": 14.81,
        "ci_upper": 25.53,
        "ci_width": 10.73
      },
      "history": {
        "accuracy": 16.41,
        "n": 256,
        "ci_lower": 12.38,
        "ci_upper": 21.44,
        "ci_width": 9.06
      },
      "administration": {
        "accuracy": 21.8,
        "n": 928,
        "ci_lower": 19.26,
        "ci_upper": 24.57,
        "ci_width": 5.31
      },
      "language": {
        "accuracy": 18.51,
        "n": 740,
        "ci_lower": 15.88,
        "ci_upper": 21.47,
        "ci_width": 5.59
      },
      "medicine": {
        "accuracy": 21.43,
        "n": 238,
        "ci_lower": 16.69,
        "ci_upper": 27.08,
        "ci_width": 10.38
      },
      "engineering": {
        "accuracy": 18.31,
        "n": 688,
        "ci_lower": 15.6,
        "ci_upper": 21.37,
        "ci_width": 5.77
      },
      "computer_science": {
        "accuracy": 17.23,
        "n": 441,
        "ci_lower": 13.99,
        "ci_upper": 21.03,
        "ci_width": 7.04
      },
      "law": {
        "accuracy": 17.0,
        "n": 818,
        "ci_lower": 14.58,
        "ci_upper": 19.73,
        "ci_width": 5.15
      },
      "mathematics": {
        "accuracy": 20.54,
        "n": 1039,
        "ci_lower": 18.19,
        "ci_upper": 23.1,
        "ci_width": 4.91
      }
    }
  },
  {
    "model": "llava-1.5-7b",
    "overall": {
      "accuracy": 14.49,
      "n": 8000,
      "ci_lower": 13.74,
      "ci_upper": 15.28,
      "ci_width": 1.54
    },
    "by_nation": {
      "india": {
        "accuracy": 14.7,
        "n": 1699,
        "ci_lower": 13.1,
        "ci_upper": 16.46,
        "ci_width": 3.37
      },
      "eu": {
        "accuracy": 15.14,
        "n": 1289,
        "ci_lower": 13.29,
        "ci_upper": 17.2,
        "ci_width": 3.91
      },
      "taiwan": {
        "accuracy": 15.05,
        "n": 557,
        "ci_lower": 12.32,
        "ci_upper": 18.26,
        "ci_width": 5.94
      },
      "japan": {
        "accuracy": 13.43,
        "n": 2557,
        "ci_lower": 12.16,
        "ci_upper": 14.81,
        "ci_width": 2.64
      },
      "south_korea": {
        "accuracy": 14.64,
        "n": 1898,
        "ci_lower": 13.12,
        "ci_upper": 16.3,
        "ci_width": 3.18
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 13.04,
        "n": 207,
        "ci_lower": 9.12,
        "ci_upper": 18.31,
        "ci_width": 9.19
      },
      "philosophy": {
        "accuracy": 14.96,
        "n": 127,
        "ci_lower": 9.79,
        "ci_upper": 22.19,
        "ci_width": 12.4
      },
      "earth_science": {
        "accuracy": 14.02,
        "n": 107,
        "ci_lower": 8.68,
        "ci_upper": 21.85,
        "ci_width": 13.17
      },
      "psychology": {
        "accuracy": 8.4,
        "n": 119,
        "ci_lower": 4.63,
        "ci_upper": 14.78,
        "ci_width": 10.15
      },
      "economics": {
        "accuracy": 17.14,
        "n": 463,
        "ci_lower": 13.98,
        "ci_upper": 20.84,
        "ci_width": 6.86
      },
      "biology": {
        "accuracy": 13.98,
        "n": 899,
        "ci_lower": 11.87,
        "ci_upper": 16.4,
        "ci_width": 4.53
      },
      "geography": {
        "accuracy": 13.26,
        "n": 181,
        "ci_lower": 9.08,
        "ci_upper": 18.97,
        "ci_width": 9.9
      },
      "physics": {
        "accuracy": 15.19,
        "n": 540,
        "ci_lower": 12.41,
        "ci_upper": 18.46,
        "ci_width": 6.05
      },
      "politics": {
        "accuracy": 12.44,
        "n": 209,
        "ci_lower": 8.63,
        "ci_upper": 17.6,
        "ci_width": 8.97
      },
      "history": {
        "accuracy": 14.45,
        "n": 256,
        "ci_lower": 10.67,
        "ci_upper": 19.28,
        "ci_width": 8.61
      },
      "administration": {
        "accuracy": 16.0,
        "n": 928,
        "ci_lower": 13.78,
        "ci_upper": 18.5,
        "ci_width": 4.72
      },
      "language": {
        "accuracy": 15.0,
        "n": 740,
        "ci_lower": 12.61,
        "ci_upper": 17.75,
        "ci_width": 5.14
      },
      "medicine": {
        "accuracy": 14.71,
        "n": 238,
        "ci_lower": 10.77,
        "ci_upper": 19.77,
        "ci_width": 9.0
      },
      "engineering": {
        "accuracy": 14.68,
        "n": 688,
        "ci_lower": 12.23,
        "ci_upper": 17.52,
        "ci_width": 5.29
      },
      "computer_science": {
        "accuracy": 15.65,
        "n": 441,
        "ci_lower": 12.56,
        "ci_upper": 19.34,
        "ci_width": 6.78
      },
      "law": {
        "accuracy": 14.66,
        "n": 818,
        "ci_lower": 12.4,
        "ci_upper": 17.25,
        "ci_width": 4.85
      },
      "mathematics": {
        "accuracy": 12.48,
        "n": 1039,
        "ci_lower": 10.61,
        "ci_upper": 14.63,
        "ci_width": 4.02
      }
    }
  },
  {
    "model": "LLaVA-NeXT-Video-7B-DPO-hf",
    "overall": {
      "accuracy": 13.63,
      "n": 8000,
      "ci_lower": 12.9,
      "ci_upper": 14.4,
      "ci_width": 1.5
    },
    "by_nation": {
      "india": {
        "accuracy": 14.7,
        "n": 1699,
        "ci_lower": 13.1,
        "ci_upper": 16.46,
        "ci_width": 3.37
      },
      "eu": {
        "accuracy": 15.14,
        "n": 1289,
        "ci_lower": 13.29,
        "ci_upper": 17.2,
        "ci_width": 3.91
      },
      "taiwan": {
        "accuracy": 15.05,
        "n": 557,
        "ci_lower": 12.32,
        "ci_upper": 18.26,
        "ci_width": 5.94
      },
      "japan": {
        "accuracy": 13.43,
        "n": 2557,
        "ci_lower": 12.16,
        "ci_upper": 14.81,
        "ci_width": 2.64
      },
      "south_korea": {
        "accuracy": 14.64,
        "n": 1898,
        "ci_lower": 13.12,
        "ci_upper": 16.3,
        "ci_width": 3.18
      }
    },
    "by_task": {
      "chemistry": {
        "accuracy": 13.04,
        "n": 207,
        "ci_lower": 9.12,
        "ci_upper": 18.31,
        "ci_width": 9.19
      },
      "philosophy": {
        "accuracy": 14.96,
        "n": 127,
        "ci_lower": 9.79,
        "ci_upper": 22.19,
        "ci_width": 12.4
      },
      "earth_science": {
        "accuracy": 14.02,
        "n": 107,
        "ci_lower": 8.68,
        "ci_upper": 21.85,
        "ci_width": 13.17
      },
      "psychology": {
        "accuracy": 8.4,
        "n": 119,
        "ci_lower": 4.63,
        "ci_upper": 14.78,
        "ci_width": 10.15
      },
      "economics": {
        "accuracy": 17.14,
        "n": 463,
        "ci_lower": 13.98,
        "ci_upper": 20.84,
        "ci_width": 6.86
      },
      "biology": {
        "accuracy": 13.98,
        "n": 899,
        "ci_lower": 11.87,
        "ci_upper": 16.4,
        "ci_width": 4.53
      },
      "geography": {
        "accuracy": 13.26,
        "n": 181,
        "ci_lower": 9.08,
        "ci_upper": 18.97,
        "ci_width": 9.9
      },
      "physics": {
        "accuracy": 15.19,
        "n": 540,
        "ci_lower": 12.41,
        "ci_upper": 18.46,
        "ci_width": 6.05
      },
      "politics": {
        "accuracy": 12.44,
        "n": 209,
        "ci_lower": 8.63,
        "ci_upper": 17.6,
        "ci_width": 8.97
      },
      "history": {
        "accuracy": 14.45,
        "n": 256,
        "ci_lower": 10.67,
        "ci_upper": 19.28,
        "ci_width": 8.61
      },
      "administration": {
        "accuracy": 16.0,
        "n": 928,
        "ci_lower": 13.78,
        "ci_upper": 18.5,
        "ci_width": 4.72
      },
      "language": {
        "accuracy": 15.0,
        "n": 740,
        "ci_lower": 12.61,
        "ci_upper": 17.75,
        "ci_width": 5.14
      },
      "medicine": {
        "accuracy": 14.71,
        "n": 238,
        "ci_lower": 10.77,
        "ci_upper": 19.77,
        "ci_width": 9.0
      },
      "engineering": {
        "accuracy": 14.68,
        "n": 688,
        "ci_lower": 12.23,
        "ci_upper": 17.52,
        "ci_width": 5.29
      },
      "computer_science": {
        "accuracy": 15.65,
        "n": 441,
        "ci_lower": 12.56,
        "ci_upper": 19.34,
        "ci_width": 6.78
      },
      "law": {
        "accuracy": 14.66,
        "n": 818,
        "ci_lower": 12.4,
        "ci_upper": 17.25,
        "ci_width": 4.85
      },
      "mathematics": {
        "accuracy": 12.48,
        "n": 1039,
        "ci_lower": 10.61,
        "ci_upper": 14.63,
        "ci_width": 4.02
      }
    }
  }
]