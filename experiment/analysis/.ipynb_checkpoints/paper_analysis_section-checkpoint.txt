%% ===========================================
%% EuraGovExam 논문 분석 섹션 (LaTeX)
%% Top Conference 수준 (ACL/EMNLP/NeurIPS 스타일)
%% ===========================================

\section{실험 결과}
\label{sec:results}

본 절에서는 23개 VLM 모델을 EuraGovExam 벤치마크에서 평가한 결과를 제시한다. 먼저 국가 및 도메인별 성능 분포를 분석하고(\S\ref{subsec:performance}), 성능 변동의 원인을 분산 분해를 통해 정량화한다(\S\ref{subsec:variance}).

\subsection{국가별 성능 분포}
\label{subsec:performance}

Table~\ref{tab:nation_performance}는 상위 5개 모델의 국가별 정확도를 Wilson 95\% 신뢰구간과 함께 제시한다. 모든 모델에서 일관된 패턴이 관찰되었다: \textbf{Taiwan이 가장 높은 정확도}를, \textbf{Japan 또는 India가 가장 낮은 정확도}를 기록하였다.

\begin{table}[t]
\centering
\small
\begin{tabular}{l|c|ccccc}
\toprule
\textbf{Model} & \textbf{Overall} & \textbf{JP} & \textbf{IN} & \textbf{KR} & \textbf{EU} & \textbf{TW} \\
\midrule
Gemini-2.5-Pro & 86.99 & 87.6 & 69.2 & 91.1 & 88.1 & \textbf{95.5} \\
& & {\scriptsize ±1.3} & {\scriptsize ±2.2} & {\scriptsize ±1.3} & {\scriptsize ±1.8} & {\scriptsize ±1.7} \\
\addlinespace[0.5ex]
o3 & 84.26 & 82.4 & 68.6 & 90.1 & 84.5 & \textbf{93.7} \\
& & {\scriptsize ±1.5} & {\scriptsize ±2.2} & {\scriptsize ±1.4} & {\scriptsize ±2.0} & {\scriptsize ±2.0} \\
\addlinespace[0.5ex]
o4-mini & 79.40 & 82.5 & 63.4 & 82.5 & 77.0 & \textbf{92.3} \\
& & {\scriptsize ±1.5} & {\scriptsize ±2.3} & {\scriptsize ±1.7} & {\scriptsize ±2.3} & {\scriptsize ±2.2} \\
\addlinespace[0.5ex]
GPT-4o & 42.04 & \underline{26.0} & 41.0 & 33.3 & 63.7 & \textbf{66.7} \\
& & {\scriptsize ±1.7} & {\scriptsize ±2.3} & {\scriptsize ±2.1} & {\scriptsize ±2.6} & {\scriptsize ±3.9} \\
\addlinespace[0.5ex]
Claude-4 & 63.29 & \underline{45.9} & 62.5 & 62.4 & 76.4 & \textbf{87.3} \\
& & {\scriptsize ±1.9} & {\scriptsize ±2.3} & {\scriptsize ±2.2} & {\scriptsize ±2.3} & {\scriptsize ±2.8} \\
\bottomrule
\end{tabular}
\caption{상위 모델의 국가별 정확도 (\%). Wilson 95\% CI 표시. \textbf{굵은 글씨}: 최고 성능, \underline{밑줄}: 최저 성능. JP=Japan, IN=India, KR=South Korea, TW=Taiwan.}
\label{tab:nation_performance}
\end{table}

특히 주목할 점은 \textbf{국가 간 성능 격차의 크기}이다. GPT-4o의 경우 Taiwan(66.7\%)과 Japan(26.0\%) 간 \textbf{40.7\%p}의 극단적인 차이가 관찰되었으며, 이는 동일 모델 내에서 국가에 따라 성능이 2.5배 이상 차이날 수 있음을 시사한다. 이러한 격차는 단순히 언어 차이로 설명되지 않는다---Japan과 Taiwan 모두 한자 문화권임에도 불구하고 Taiwan에서 일관되게 높은 성능을 보이기 때문이다.

\subsection{분산 분해: 국가 vs 도메인}
\label{subsec:variance}

VLM 성능 변동의 원인을 정량화하기 위해 각 모델에 대해 국가별 분산($\sigma^2_{\text{nation}}$)과 도메인별 분산($\sigma^2_{\text{task}}$)을 계산하였다. Figure~\ref{fig:variance}에서 확인할 수 있듯이, 23개 모델 전체에서 \textbf{국가가 도메인보다 평균 2.52배 더 큰 분산}을 유발한다:

\begin{equation}
\frac{\bar{\sigma}^2_{\text{nation}}}{\bar{\sigma}^2_{\text{task}}} = \frac{137.44}{54.44} = 2.52
\end{equation}

이 결과는 VLM 성능이 ``무엇을 묻는가'' (도메인)보다 \textbf{``어디서 출제되었는가'' (국가/스크립트)}에 더 민감하게 반응한다는 것을 의미한다. 특히 저성능 모델에서 이 경향이 두드러진다: Gemini-2.5-flash-lite의 경우 국가 분산(702.1)이 도메인 분산(127.4)의 \textbf{5.5배}에 달했다.

\paragraph{통계적 유의성} 모델 간 성능 차이의 통계적 유의성을 검증하기 위해 two-proportion $z$-test를 수행하고 Bonferroni 보정($\alpha = 0.001$)을 적용하였다. 상위 10개 모델의 45개 쌍 비교 중 \textbf{44개(97.8\%)가 통계적으로 유의}하였으며, 유일한 예외는 o3(84.3\%)와 Gemini-2.5-Pro(87.0\%) 간 비교였다 ($z = 2.47$, $p = 0.014$).


%% ===========================================
\section{실패 분류 체계}
\label{sec:taxonomy}

성능 격차의 근본 원인을 파악하기 위해, 어려운 문제를 대상으로 체계적인 실패 원인 분석을 수행하였다.

\subsection{분석 방법론}

\paragraph{문제 선정} \S\ref{subsec:performance}에서 확인된 난이도 패턴을 기반으로 177개 문제를 층화 표집하였다. 구체적으로, 어려운 국가(Japan, India)와 어려운 도메인(Earth Science, Geography, Physics, Mathematics 등)의 조합에서 60개, 둘 중 하나만 해당하는 경우 80개, 나머지 37개를 무작위 추출하였다.

\paragraph{분류 체계} 각 문제 이미지에 대해 10개 실패 카테고리 중 1차 원인 1개와 2차 원인 최대 2개를 분류하였다:
\begin{enumerate}[noitemsep,topsep=0pt]
    \item \textsc{Non-Latin Script}: 세로쓰기, CJK 문자, 데바나가리 등 비라틴 스크립트
    \item \textsc{OCR/Text}: 일반적인 텍스트 인식 실패
    \item \textsc{Math/Symbol}: 수학 수식 및 기호 해석 오류
    \item \textsc{Pure Reasoning}: 시각적 문제 없이 순수 추론/지식 부족
    \item \textsc{Diagram}: 다이어그램, 회로도, 플로우차트 이해 실패
    \item \textsc{Table/Chart}: 표 또는 차트 구조 파악 실패
    \item \textsc{Figure-Text}: 그림과 텍스트 간 참조 관계 오류
    \item \textsc{Code-Switch}: 다국어 혼용 처리 실패
    \item \textsc{Layout}: 다단 또는 복잡한 레이아웃 혼동
    \item \textsc{Quality}: 저해상도, 흐림 등 이미지 품질 문제
\end{enumerate}

분류는 Gemini-2.0-Flash를 사용하여 자동화하였으며, 176개 문제에서 유효한 결과를 얻었다.

\subsection{실패 원인 분포}

Table~\ref{tab:failure_dist}는 1차 실패 원인의 전체 분포를 보여준다. \textbf{비라틴 스크립트 관련 실패가 전체의 40.3\%}로 압도적인 1위를 차지하였다.

\begin{table}[t]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Failure Category} & \textbf{Count} & \textbf{\%} \\
\midrule
\textsc{Non-Latin Script} & 71 & 40.3 \\
\textsc{OCR/Text Recognition} & 31 & 17.6 \\
\textsc{Math/Symbol} & 25 & 14.2 \\
\textsc{Pure Reasoning} & 20 & 11.4 \\
\textsc{Diagram Understanding} & 12 & 6.8 \\
\textsc{Table/Chart Structure} & 11 & 6.3 \\
\textsc{Figure-Text Alignment} & 4 & 2.3 \\
\textsc{Others} & 2 & 1.1 \\
\midrule
\textbf{Total} & \textbf{176} & \textbf{100.0} \\
\bottomrule
\end{tabular}
\caption{1차 실패 원인 분포. 시각적 인식 관련 실패(상위 6개 카테고리 중 Pure Reasoning 제외)가 전체의 87.5\%를 차지한다.}
\label{tab:failure_dist}
\end{table}

주목할 점은 \textbf{시각적 인식 관련 실패}(\textsc{Non-Latin} + \textsc{OCR} + \textsc{Diagram} + \textsc{Table} + \textsc{Math})가 전체의 \textbf{85.2\%}를 차지하는 반면, \textbf{순수 추론/지식 부족은 11.4\%}에 불과하다는 것이다. 이는 현재 VLM의 주요 병목이 ``생각하는 능력''이 아닌 \textbf{``보는 능력''}에 있음을 강하게 시사한다.

\subsection{국가별 실패 패턴: 병목 지점의 전이}
\label{subsec:bottleneck_shift}

가장 중요한 발견은 \textbf{실패 원인이 국가에 따라 체계적으로 다르다}는 점이다. Table~\ref{tab:nation_failure}는 국가별 1차 실패 원인 분포를 보여준다.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|rrrrr}
\toprule
\textbf{Nation} & \textbf{Script} & \textbf{OCR} & \textbf{Math} & \textbf{Reason} & \textbf{Other} \\
\midrule
Japan ($n$=65) & 43.1 & 4.6 & 27.7 & 3.1 & 21.5 \\
India ($n$=32) & 56.3 & 40.6 & 3.1 & 0.0 & 0.0 \\
S.Korea ($n$=47) & 51.1 & 10.6 & 12.8 & 10.6 & 14.9 \\
EU ($n$=25) & 0.0 & 24.0 & 0.0 & \textbf{48.0} & 28.0 \\
Taiwan ($n$=7) & 14.3 & 57.1 & 0.0 & 14.3 & 14.3 \\
\bottomrule
\end{tabular}
\caption{국가별 1차 실패 원인 분포 (\%). Script=Non-Latin Script, Reason=Pure Reasoning. EU에서만 순수 추론 실패가 지배적이다.}
\label{tab:nation_failure}
\end{table}

이 결과는 우리가 \textbf{``Regional Bottleneck Shift''}라 명명한 현상을 명확히 보여준다:

\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{CJK + India} (비라틴 스크립트 국가): 실패의 50--97\%가 스크립트/OCR 관련 $\rightarrow$ \colorbox{yellow!30}{\textbf{Perception Bottleneck}}
    \item \textbf{EU} (라틴 스크립트 국가): 실패의 48\%가 순수 추론/지식 부족 $\rightarrow$ \colorbox{cyan!30}{\textbf{Reasoning Bottleneck}}
\end{itemize}

\noindent 즉, \textbf{동일한 VLM이라도 Japan 문제에서는 ``글자를 읽지 못해서'' 실패하고, EU 문제에서는 ``읽었지만 답을 몰라서'' 실패}한다. 이는 macro-average 성능만으로는 포착할 수 없는 근본적인 실패 모드의 차이이다.

\subsection{시각적 요소 분석}

분석 대상 문제의 시각적 특성을 정량화한 결과, 어려운 문제의 \textbf{38.6\%가 수학 수식/기호}를 포함하고 있었다 (Table~\ref{tab:visual_elements}). 이는 수식 렌더링과 기호 해석이 VLM 성능의 핵심 요인임을 시사한다.

\begin{table}[t]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Visual Element} & \textbf{Count} & \textbf{\%} \\
\midrule
수학 수식/기호 & 68 & 38.6 \\
표/테이블 & 26 & 14.8 \\
다이어그램 & 24 & 13.6 \\
손글씨 & 0 & 0.0 \\
\bottomrule
\end{tabular}
\caption{분석 대상 문제의 시각적 요소 분포 ($n$=176).}
\label{tab:visual_elements}
\end{table}


%% ===========================================
\section{논의: Script Bias 가설}
\label{sec:discussion}

\subsection{발견의 통합}

위 분석 결과를 종합하면 다음과 같은 핵심 통찰을 도출할 수 있다:

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=주요 발견]
\begin{enumerate}[noitemsep]
    \item \textbf{Finding 1}: 국가가 도메인보다 2.5배 더 큰 성능 분산을 유발한다.
    \item \textbf{Finding 2}: 실패 원인의 85\%가 시각적 인식 관련이며, 이 중 47\%가 비라틴 스크립트 문제이다.
    \item \textbf{Finding 3}: 비라틴 스크립트 국가에서는 Perception이 병목이지만, 라틴 스크립트 국가에서는 Reasoning이 병목이다.
\end{enumerate}
\end{tcolorbox}

이를 바탕으로 우리는 \textbf{Script Bias 가설}을 제안한다:

\begin{quote}
\textit{현재 VLM은 학습 데이터의 라틴 스크립트 편향으로 인해, 비라틴 문자(특히 세로쓰기 일본어, 복잡한 한자, 데바나가리)에서 체계적으로 낮은 시각적 인식 성능을 보인다. 이로 인해 동일한 난이도의 문제라도 스크립트에 따라 실패 모드가 근본적으로 달라진다.}
\end{quote}

\subsection{함의}

\paragraph{벤치마킹 함의} 다국어 VLM 평가 시 macro-average는 지역별 성능 격차를 은폐한다. 본 연구는 \textbf{스크립트/국가별 층화 보고}의 필요성을 강조한다. 특히 Japan에서 26\%의 정확도를 보이는 모델이 전체 평균 42\%로 보고되는 것은 실질적인 다국어 성능을 과대평가하는 것이다.

\paragraph{학습 데이터 함의} 비라틴 스크립트 문서, 특히 세로쓰기와 복잡한 레이아웃을 포함하는 데이터의 학습 비중 확대가 필요하다. 현재의 Perception Bottleneck은 모델 아키텍처보다 \textbf{데이터 불균형}에서 기인할 가능성이 높다.

\paragraph{모델 설계 함의} CJK/India 성능 향상을 위해서는 reasoning 능력 강화보다 \textbf{인식 단계의 개선}이 더 효과적일 수 있다. OCR 전처리 파이프라인이나 스크립트별 특화 인코더 등의 접근이 유망하다.

\subsection{제한점}

본 분석에는 다음과 같은 제한점이 있다:

\begin{enumerate}[noitemsep]
    \item \textbf{자동 분류}: 실패 원인 분류에 Gemini-2.0-Flash를 사용하였으며, 인간 검증은 수행하지 않았다. 향후 소규모 인간 평가($n \geq 50$)를 통한 분류 신뢰도 검증이 필요하다.
    
    \item \textbf{인과 추론의 한계}: 스크립트, 언어 난이도, 문화적 특성이 혼재되어 있어, 스크립트만의 순수 효과를 완전히 분리하기 어렵다.
    
    \item \textbf{개입 실험 부재}: Clean-text 상한선이나 OCR-assisted 파이프라인 등의 개입 실험을 통해 Perception Gap을 직접 정량화하는 후속 연구가 필요하다.
\end{enumerate}


%% ===========================================
%% 필요한 패키지 (preamble에 추가)
%% ===========================================
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{enumitem}
% \usepackage{xcolor}
% \usepackage{tcolorbox}
% \usepackage{amsmath}
